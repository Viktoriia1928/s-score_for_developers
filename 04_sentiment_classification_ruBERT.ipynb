{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ –æ –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–∞—Ö  –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **–ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å ruBERT-large —Å Hugging Face –æ—Ç SberDevices**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "K6ARBrydipsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **–ü–ª–∞–Ω –æ–±—É—á–µ–Ω–∏—è –≤—ã–≥–ª—è–¥–∏—Ç —Ç–∞–∫:**\n",
        "\n",
        "1.   –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π, —Ä–∞–∑–º–µ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö)\n",
        "2.   –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\n",
        "3.   –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏\n",
        "4.   –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —á–µ—Ä–µ–∑ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é —Å Optuna\n",
        "5.   –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Ç—Å–≤–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9xyB80BKjyqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö**"
      ],
      "metadata": {
        "id": "6W2FPoqDlMCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "–î–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (—Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏) –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è —Ä–µ–π—Ç–∏–Ω–≥–æ–º –æ—Ç–∑—ã–≤–∞ - –ø–æ–ª–µ ***rating***. –ü–æ–ª–µ ***rating*** –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è –æ—Ç **1** –¥–æ **5** (–∑–≤–µ–∑–¥–æ—á–∫–∏ –≤ –æ—Ç–∑—ã–≤–µ). –ú—ã –ø–µ—Ä–µ–≤–µ–¥–µ–º –∏—Ö –≤ –º–µ—Ç–∫–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: –æ—Ç–∑—ã–≤—ã —Å —Ä–µ–π—Ç–∏–Ω–≥–æ–º **1** –∏–ª–∏ **2** —Å—á–∏—Ç–∞—é—Ç—Å—è **–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–º–∏** (*-1*, –∑–∞–∫–æ–¥–∏—Ä—É–µ–º –∫–∞–∫ –∫–ª–∞—Å—Å *0*), —Ä–µ–π—Ç–∏–Ω–≥ **3** ‚Äî **–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π** (*0*, –∫–ª–∞—Å—Å *1*), –∞ **4** –∏–ª–∏ **5** ‚Äî **–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π** (*+1*, –∫–ª–∞—Å—Å *2*)."
      ],
      "metadata": {
        "id": "5-iHbpb3lkD2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QokTZcj8iGzF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('all_reviews_bert.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Map ratings (1-5) to sentiment labels (0=negative, 1=neutral, 2=positive)\n",
        "def rating_to_label(rating):\n",
        "    if rating <= 2.0:\n",
        "        return 0\n",
        "    elif rating == 3.0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 2\n",
        "\n",
        "df['sentiment_label'] = df['rating'].apply(rating_to_label).astype(int)"
      ],
      "metadata": {
        "id": "NaJzd19PbX7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment class distribution\n",
        "class_counts = df['sentiment_label'].value_counts()\n",
        "print(\"Class distribution:\")\n",
        "for label, count in class_counts.items():\n",
        "    print(f\"{label}: {count}\")"
      ],
      "metadata": {
        "id": "IgcGwvdgqyeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–í–∏–¥–∏–º, —á—Ç–æ –∫–ª–∞—Å—Å—ã —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –Ω–µ—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ - –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è —è–≤–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å.\n",
        "\n",
        "–î–ª—è –Ω–∞–¥—ë–∂–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ä–∞–∑–±–∏–≤–∫—É –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ –∫–ª–∞—Å—Å–æ–≤. –û—Ç–ª–æ–∂–∏–º **20% –¥–∞–Ω–Ω—ã—Ö** –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ (holdout) –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏, –∞ –æ—Å—Ç–∞–≤—à–∏–µ—Å—è 80% –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.\n"
      ],
      "metadata": {
        "id": "XkXaf96jrjr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting dataset on training and test sample (startified by sentiment)\n",
        "train_df, test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    stratify=df['sentiment_label'],\n",
        "    random_state=42)\n",
        "\n",
        "print(f\"Train sample size: {len(train_df)}, Test sample size: {len(test_df)}\")\n",
        "\n",
        "# Reseting index\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "ita8Iu9Iq1Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é Natasha**"
      ],
      "metadata": {
        "id": "gA62rhLfyhGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ü–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ –≤—ã–ø–æ–ª–Ω–∏–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞: –æ—á–∏—Å—Ç–∏–º –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø—Ä–∏–≤–µ–¥–µ–º —Å–ª–æ–≤–∞ –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ (–ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è). –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫—É **Natasha**, –∫–æ—Ç–æ—Ä–∞—è –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é —Å —É—á–µ—Ç–æ–º –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏.\n",
        "\n",
        "–®–∞–≥–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ—Ç–∑—ã–≤–∞:\n",
        "*   –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (—Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —Å–ª–æ–≤–∞ –∏ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è)\n",
        "*   –£–¥–∞–ª–µ–Ω–∏–µ –∏–ª–∏ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏, —á–∏—Å–µ–ª –∏ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤\n",
        "*   –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Å–ª–æ–≤ –∫ –Ω–∞—á–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ (–ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è) —Å –ø–æ–º–æ—â—å—é –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ Natasha\n",
        "*   –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
        "\n",
        "\n",
        "–ü–æ—Å–ª–µ —ç—Ç–∏—Ö —à–∞–≥–æ–≤ —Ç–µ–∫—Å—Ç —Å—Ç–∞–Ω–µ—Ç –±–æ–ª–µ–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º, —á—Ç–æ –æ–±–ª–µ–≥—á–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–º–æ–¥–µ–ª—å –Ω–µ –±—É–¥–µ—Ç —É—á–∏—Ç—ã–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–∏—è –≤ —Ñ–æ—Ä–º–∞—Ö —Å–ª–æ–≤, –∞ –ª–∏—à—å –∏—Ö –æ—Å–Ω–æ–≤—ã). –ù–∏–∂–µ –æ–ø—Ä–µ–¥–µ–ª–∏–º —Ñ—É–Ω–∫—Ü–∏—é **preprocess_text** –¥–ª—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏. –ó–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω–∏–º –µ—ë –∫–æ –≤—Å–µ–º –æ—Ç–∑—ã–≤–∞–º –≤ –æ–±—É—á–∞—é—â–µ–π –∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤—ã–±–æ—Ä–∫–∞—Ö."
      ],
      "metadata": {
        "id": "0y1AB10KynF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install natasha"
      ],
      "metadata": {
        "id": "kbGCQpyPwDeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from natasha import Segmenter, NewsEmbedding, NewsMorphTagger, MorphVocab, Doc\n",
        "import re\n",
        "\n",
        "segmenter = Segmenter()\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "morph_vocab = MorphVocab()\n",
        "\n",
        "def lemmatize_text(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^–∞-—è—ë–ê-–Ø–Åa-zA-Z\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    doc = Doc(text)\n",
        "    doc.segment(segmenter)\n",
        "    doc.tag_morph(morph_tagger)\n",
        "    for token in doc.tokens:\n",
        "        token.lemmatize(morph_vocab)\n",
        "\n",
        "    lemmas = \" \".join([token.lemma for token in doc.tokens if token.lemma])\n",
        "    return lemmas\n",
        "\n",
        "train_df['text_processed'] = train_df['full_review_text'].apply(lemmatize_text)\n",
        "test_df['text_processed']  = test_df['full_review_text'].apply(lemmatize_text)"
      ],
      "metadata": {
        "id": "D473mRW2zQBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ü—Ä–∏–º–µ—Ä: –ø–æ—Å–º–æ—Ç—Ä–∏–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø–∞—Ä—ã –æ—Ç–∑—ã–≤–æ–≤"
      ],
      "metadata": {
        "id": "y3pcnz320Oox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_indices = [0, 1]\n",
        "for idx in example_indices:\n",
        "    orig_text = train_df.loc[idx, 'full_review_text']\n",
        "    proc_text = train_df.loc[idx, 'text_processed']\n",
        "    print(f\"Example {idx}:\")\n",
        "    print(\"Original text:\", orig_text)\n",
        "    print(\"After lemmatization:\", proc_text)"
      ],
      "metadata": {
        "id": "2OvI-Vcz0Pk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. **–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏ BERT**"
      ],
      "metadata": {
        "id": "fEpJVlfp1lyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "–î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ BERT –Ω–∞–º –Ω—É–∂–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç—ã –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç ‚Äì —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏—Ö —Å –ø–æ–º–æ—â—å—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞. –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –æ—Ç –º–æ–¥–µ–ª–∏ ai-forever/ruBert-large –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ü§ó Transformers. –û–Ω –ø—Ä–∏–≤–µ–¥–µ—Ç –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ (–∏–ª–∏ —á–∞—Å—Ç—å —Å–ª–æ–≤–∞) –∫ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—É –∏–∑ —Å–ª–æ–≤–∞—Ä—è –º–æ–¥–µ–ª–∏.\n",
        "\n",
        "- –ó–∞–¥–∞–¥–∏–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ MAX_LEN = 256 —Ç–æ–∫–µ–Ω–æ–≤. –ï—Å–ª–∏ –æ—Ç–∑—ã–≤ –¥–ª–∏–Ω–Ω–µ–µ, –æ–Ω –±—É–¥–µ—Ç —É—Å–µ—á–µ–Ω –¥–æ 256 —Ç–æ–∫–µ–Ω–æ–≤, –µ—Å–ª–∏ –∫–æ—Ä–æ—á–µ ‚Äì –±—É–¥–µ—Ç –¥–æ–ø–æ–ª–Ω–µ–Ω —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏ –∑–∞–ø–æ–ª–Ω–∏—Ç–µ–ª—è–º–∏ –ø—Ä–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ –±–∞—Ç—á–∞.\n",
        "- –ß—Ç–æ–±—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã, –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è DataCollatorWithPadding. –û–Ω –±—É–¥–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–∞–¥–¥–∏—Ä–æ–≤–∞—Ç—å (–¥–æ–ø–æ–ª–Ω—è—Ç—å) –º–µ–Ω—å—à–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–æ –¥–ª–∏–Ω—ã —Å–∞–º–æ–π –¥–ª–∏–Ω–Ω–æ–π –≤ —Ç–µ–∫—É—â–µ–º –±–∞—Ç—á–µ. –¢–∞–∫ –º—ã –Ω–µ —Ä–∞—Å—Ö–æ–¥—É–µ–º –ª–∏—à–Ω—é—é –ø–∞–º—è—Ç—å –Ω–∞ –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–æ 256, –∞ —Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–∏ –±–∞—Ç—á–∞.\n",
        "- –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –∫–∞—Å—Ç–æ–º–Ω—ã–π –∫–ª–∞—Å—Å Dataset –¥–ª—è –Ω–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π –ø–æ –∏–Ω–¥–µ–∫—Å—É –±—É–¥–µ—Ç –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–∑—ã–≤ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –º–µ—Ç–∫—É. –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç –ª–µ–≥–∫–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å DataLoader –¥–ª—è –∏—Ç–µ—Ä–∞—Ü–∏–∏ –ø–æ –¥–∞–Ω–Ω—ã–º –±–∞—Ç—á–∞–º–∏."
      ],
      "metadata": {
        "id": "8eBv-9u61wpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Initializing ruBERT-large tokenizator\n",
        "model_name = \"ai-forever/ruBert-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "token_max_len = 256\n",
        "\n",
        "# Tokenizing training and test sample texts\n",
        "train_encodings = tokenizer(\n",
        "    list(train_df['text_processed']),\n",
        "    truncation=True,\n",
        "    padding=False,\n",
        "    max_length=token_max_len\n",
        ")\n",
        "\n",
        "test_encodings = tokenizer(\n",
        "    list(test_df['text_processed']),\n",
        "    truncation=True,\n",
        "    padding=False,\n",
        "    max_length=token_max_len\n",
        ")\n",
        "\n",
        "# Prepare torch Dataset objects\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        # vocabulary with keys 'input_ids', 'attention_mask'\n",
        "        self.encodings = encodings\n",
        "        # labels list\n",
        "        self.labels = labels\n",
        "    def __getitem__(self, idx):\n",
        "        # Taking tokenized representations for this index\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        # Adding labels\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Creating Dataset objects for training and test samples\n",
        "train_dataset = ReviewDataset(train_encodings, train_df['sentiment_label'].tolist())\n",
        "test_dataset = ReviewDataset(test_encodings, test_df['sentiment_label'].tolist())\n",
        "\n",
        "# Creating collator for dynamic padding batches\n",
        "collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "eO-6hk3g0UM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ü—Ä–∏–º–µ—Ä: –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º —Ä–∞–±–æ—Ç—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ"
      ],
      "metadata": {
        "id": "OGwpSmKK4xTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = train_df.loc[0, 'text_processed']\n",
        "tokens = tokenizer.tokenize(sample_text)\n",
        "input_ids = train_encodings['input_ids'][0]\n",
        "\n",
        "print(\"Lemmatized text:\")\n",
        "print(sample_text)\n",
        "print(\"\\nBERT tokens:\")\n",
        "print(tokens)\n",
        "print(\"\\nTokens IDs:\")\n",
        "print(input_ids)"
      ],
      "metadata": {
        "id": "fKDfPNaD4nRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è: –º–æ–¥–µ–ª—å, –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã**"
      ],
      "metadata": {
        "id": "ap2YHCDR45mT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å **BertForSequenceClassification** (–Ω–∞ –æ—Å–Ω–æ–≤–µ ruBERT-large) —Å –≤—ã—Ö–æ–¥–Ω—ã–º —Å–ª–æ–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ 3 –∫–ª–∞—Å—Å–∞. –ú–æ–¥–µ–ª—å –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–∞–¥ [CLS]-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º –ª–∏–Ω–µ–π–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏ softmax –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –∫–ª–∞—Å—Å–æ–≤.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#### **–ü–ª–∞–Ω –æ–±—É—á–µ–Ω–∏—è:**\n",
        "\n",
        "- **–ú–æ–¥–µ–ª—å:** BertForSequenceClassification.from_pretrained('ai-forever/ruBert-large', num_labels=3).\n",
        "- **–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä:** AdamW (–∞–¥–∞–º —Å –¥–µ–∫–µ–π –Ω–∞ –≤–µ—Å–∞—Ö) ‚Äì —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è BERT. –ú—ã –Ω–∞—Å—Ç—Ä–æ–∏–º weight decay –¥–ª—è –≤—Å–µ—Ö –≤–µ—Å–æ–≤ –∫—Ä–æ–º–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ —Å–º–µ—â–µ–Ω–∏—è –∏ LayerNorm (–∏—Ö –æ–±—ã—á–Ω–æ –Ω–µ –ø–æ–¥–≤–µ—Ä–≥–∞—é—Ç L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏).\n",
        "- **Scheduler:** –ª–∏–Ω–µ–π–Ω—ã–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø—Ä–æ–≥—Ä–µ–≤–æ–º (get_linear_schedule_with_warmup) ‚Äì —Å–Ω–∞—á–∞–ª–∞ –Ω–µ–±–æ–ª—å—à–æ–µ warmup_ratio —ç–ø–æ—Ö –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è —Ä–∞–∑–æ–≥—Ä–µ–≤–∞ (–Ω–∏–∑–∫–∏–π learning rate), –∑–∞—Ç–µ–º –ª–∏–Ω–µ–π–Ω–æ–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ learning rate –¥–æ 0 –∫ –∫–æ–Ω—Ü—É –æ–±—É—á–µ–Ω–∏—è.\n",
        "- **Loss-—Ñ—É–Ω–∫—Ü–∏—è:** –±—É–¥–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å –æ–±—ã—á–Ω–æ–π –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏–µ–π (CrossEntropyLoss) –∏ Focal Loss, –∫–æ—Ç–æ—Ä–∞—è –º–æ–∂–µ—Ç —É–ª—É—á—à–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Ñ–æ–∫—É—Å–∏—Ä—É—è—Å—å –Ω–∞ —Ç—Ä—É–¥–Ω–æ–∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö. –¢–∞–∫–∂–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –¥–≤–∞ –ø–æ–¥—Ö–æ–¥–∞ –∫ —É—á—ë—Ç—É –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞:\n",
        "    - –í–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ (class weights) –≤ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å.\n",
        "    - WeightedRandomSampler ‚Äì –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –≤—ã–±–æ—Ä–∫–∏ –ø—É—Ç—ë–º –±–æ–ª–µ–µ —á–∞—Å—Ç–æ–≥–æ –≤—ã–±–æ—Ä–∞ —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤ –ø—Ä–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ –±–∞—Ç—á–µ–π.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "sXMXSV2E4_bz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **–ù–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç –ø–æ–¥–æ–±—Ä–∞—Ç—å —Ä—è–¥ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:**\n",
        "- ***learning_rate*** ‚Äì —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (–æ—Ç 1e-5 –¥–æ 5e-5, –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–π –º–∞—Å—à—Ç–∞–±).\n",
        "- ***batch_size*** ‚Äì —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (8, 16 –∏–ª–∏ 32).\n",
        "- ***weight_decay*** ‚Äì –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ (0.0 –¥–æ 0.1).\n",
        "- ***epochs*** ‚Äì –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è (–æ—Ç 2 –¥–æ 6).\n",
        "- ***warmup_ratio*** ‚Äì –¥–æ–ª—è —Ä–∞–∑–æ–≥—Ä–µ–≤–∞ (0.0, 0.1 –∏–ª–∏ 0.2 –æ—Ç –≤—Å–µ–≥–æ —á–∏—Å–ª–∞ —à–∞–≥–æ–≤).\n",
        "- ***loss_fn*** ‚Äì —Ç–∏–ø —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å: ce (CrossEntropy) –∏–ª–∏ focal (FocalLoss).\n",
        "- ***balance_strategy*** ‚Äì —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –±–æ—Ä—å–±—ã —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º: class_weights –∏–ª–∏ sampler."
      ],
      "metadata": {
        "id": "wAmoOE756MXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ß—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —ç—Ç–∏—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –º—ã –ø—Ä–∏–º–µ–Ω–∏–º –ø–æ–∏—Å–∫ —Å –ø–æ–º–æ—â—å—é **Optuna**. –í –∫–∞—á–µ—Å—Ç–≤–µ —Ü–µ–ª–µ–≤–æ–π –º–µ—Ç—Ä–∏–∫–∏ –±—É–¥–µ–º –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å **Macro F1** –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏. –ú—ã —Ä–∞–∑–¥–µ–ª–∏–º –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ 5 —Ñ–æ–ª–¥–æ–≤ (Stratified K-Fold) –∏ –±—É–¥–µ–º –≤ —Ä–∞–º–∫–∞—Ö –∫–∞–∂–¥–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ 4 —Ñ–æ–ª–¥–∞—Ö –∏ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å Macro F1 –Ω–∞ –æ—Å—Ç–∞–≤—à–µ–º—Å—è —Ñ–æ–ª–¥–µ, —É—Å—Ä–µ–¥–Ω—è—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ 5 —Ñ–æ–ª–¥–∞–º. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, Optuna –±—É–¥–µ—Ç –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∫–∞–∂–¥–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
        "\n",
        "–í–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∫–∞–∂–¥–æ–º —Ñ–æ–ª–¥–µ —Ä–µ–∞–ª–∏–∑—É–µ–º Early Stopping: –µ—Å–ª–∏ Macro F1 –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–µ —É–ª—É—á—à–∞–µ—Ç—Å—è –≤ —Ç–µ—á–µ–Ω–∏–µ 2 —ç–ø–æ—Ö –ø–æ–¥—Ä—è–¥, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —ç—Ç–æ–º —Ñ–æ–ª–¥–µ –¥–æ—Å—Ä–æ—á–Ω–æ, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å—Å—è –∏ —Å—ç–∫–æ–Ω–æ–º–∏—Ç—å –≤—Ä–µ–º—è. –ë—É–¥–µ–º —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –Ω–∞–∏–ª—É—á—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ F1 –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–æ–ª–¥–∞.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0bznkKhf6vRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **–ù–∞—Å—Ç—Ä–æ–π–∫–∞ Optuna –∏ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏**\n",
        "\n",
        "–û–ø—Ä–µ–¥–µ–ª–∏–º —Ñ—É–Ω–∫—Ü–∏—é-—Ü–µ–ª—å –¥–ª—è Optuna, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ 5-—Ñ–æ–ª–¥–æ–≤–æ–π –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ä–µ–¥–Ω–∏–π Macro F1. –í–Ω—É—Ç—Ä–∏ –Ω–µ–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å–ª–µ–¥—É—é—â–µ–µ:\n",
        "\n",
        "- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–æ–ª–¥–∞ —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.\n",
        "- –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤—ã–±—Ä–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: –ª–∏–±–æ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ –≤ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å, –ª–∏–±–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ WeightedRandomSampler –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–∞—Ç—á–µ–π.\n",
        "- –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ linear scheduler —Å —Ä–∞–∑–æ–≥—Ä–µ–≤–æ–º.\n",
        "- –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –ø–æ —ç–ø–æ—Ö–∞–º —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º best F1 –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —É—Å–ª–æ–≤–∏–µ–º —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏.\n",
        "- –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –≤—Å–µ—Ö —Ñ–æ–ª–¥–∞—Ö ‚Äì –≤–æ–∑–≤—Ä–∞—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ Macro F1 –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."
      ],
      "metadata": {
        "id": "Mhqa5eJ473Vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "31uvHCyb46xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "4vBOcPA0_tLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "Xxa31tcyBbl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, numpy as np, optuna\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from accelerate import Accelerator\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 1. Focal-loss\n",
        "def focal_loss(logits, targets, gamma=2.0, class_weights=None):\n",
        "    log_probs = nn.functional.log_softmax(logits.float(), dim=1)\n",
        "    probs     = torch.exp(log_probs)\n",
        "    pt        = probs[range(len(targets)), targets]\n",
        "    log_pt    = log_probs[range(len(targets)), targets]\n",
        "    alpha     = class_weights[targets] if class_weights is not None else torch.ones_like(pt)\n",
        "    return (-alpha * ((1 - pt) ** gamma) * log_pt).mean()\n",
        "\n",
        "# 2. Stratified 5-fold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# array of labels\n",
        "y_all = train_df['sentiment_label'].values\n",
        "\n",
        "# 3. Optuna objective\n",
        "def objective(trial):\n",
        "    learning_rate  = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
        "    batch_size     = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
        "    weight_decay   = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n",
        "    epochs         = trial.suggest_int(\"epochs\", 2, 6)\n",
        "    warmup_ratio   = trial.suggest_categorical(\"warmup_ratio\", [0.0, 0.1, 0.2])\n",
        "    loss_fn_name   = trial.suggest_categorical(\"loss_fn\", [\"ce\", \"focal\"])\n",
        "    balance_strat  = trial.suggest_categorical(\"balance_strategy\", [\"class_weights\", \"sampler\"])\n",
        "\n",
        "    total_f1 = 0.0\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(y_all)), y_all), 1):\n",
        "\n",
        "        print(f\"Fold {fold}/5  ‚Äî  trial #{trial.number}\")\n",
        "        train_subset = torch.utils.data.Subset(train_dataset, train_idx)\n",
        "        val_subset   = torch.utils.data.Subset(train_dataset,   val_idx)\n",
        "\n",
        "        # Balancing classes\n",
        "        if balance_strat == \"sampler\":\n",
        "            class_cnt  = np.bincount(y_all[train_idx], minlength=3)\n",
        "            w_cls      = 1.0 / class_cnt\n",
        "            sample_w   = [w_cls[y] for y in y_all[train_idx]]\n",
        "            sampler    = WeightedRandomSampler(sample_w, len(sample_w), replacement=True)\n",
        "            train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size,\n",
        "                                                       sampler=sampler, collate_fn=collator)\n",
        "        else:\n",
        "            train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size,\n",
        "                                                       shuffle=True,  collate_fn=collator)\n",
        "\n",
        "        val_loader   = torch.utils.data.DataLoader(val_subset,   batch_size=batch_size,\n",
        "                                                   shuffle=False, collate_fn=collator)\n",
        "\n",
        "        # Model and optimizator\n",
        "        model = BertForSequenceClassification.from_pretrained(\n",
        "            'ai-forever/ruBert-large',\n",
        "            num_labels=3\n",
        "        )\n",
        "\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        param_groups = [\n",
        "            {\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "             \"weight_decay\": weight_decay},\n",
        "            {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "             \"weight_decay\": 0.0}\n",
        "        ]\n",
        "\n",
        "        optimizer = AdamW(param_groups, lr=learning_rate)\n",
        "\n",
        "        steps_per_epoch   = len(train_loader)\n",
        "        total_train_steps = epochs * steps_per_epoch\n",
        "        warmup_steps      = int(warmup_ratio * total_train_steps)\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_train_steps)\n",
        "\n",
        "        accelerator = Accelerator(mixed_precision=\"fp16\")\n",
        "        model, optimizer, train_loader, val_loader, scheduler = accelerator.prepare(\n",
        "            model, optimizer, train_loader, val_loader, scheduler\n",
        "        )\n",
        "\n",
        "        # loss-function\n",
        "        if loss_fn_name == \"ce\":\n",
        "            if balance_strat == \"class_weights\":\n",
        "                cnt = np.bincount(y_all[train_idx], minlength=3)\n",
        "                w   = torch.tensor((len(train_idx)/cnt)/3.0, dtype=torch.float).to(accelerator.device)\n",
        "                criterion = nn.CrossEntropyLoss(weight=w)\n",
        "            else:\n",
        "                criterion = nn.CrossEntropyLoss()\n",
        "        else:\n",
        "            cnt = np.bincount(y_all[train_idx], minlength=3) if balance_strat==\"class_weights\" else None\n",
        "            class_w = torch.tensor((len(train_idx)/cnt)/3.0, dtype=torch.float).to(accelerator.device) if cnt is not None else None\n",
        "            criterion = None\n",
        "\n",
        "        # Epochs cycle with progress bar\n",
        "        best_f1_fold, epochs_no_improve = 0.0, 0\n",
        "        epoch_bar = tqdm(range(1, epochs+1), desc=f\"Epochs (fold {fold})\", leave=False)\n",
        "\n",
        "        for epoch in epoch_bar:\n",
        "            model.train()\n",
        "            batch_bar = tqdm(train_loader, desc=f\"Train ep{epoch}\", leave=False)\n",
        "\n",
        "            total_train_loss = 0.0\n",
        "            for batch in batch_bar:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(input_ids=batch['input_ids'],\n",
        "                                attention_mask=batch['attention_mask'],\n",
        "                                token_type_ids=batch.get('token_type_ids', None))\n",
        "                logits = outputs.logits\n",
        "\n",
        "                if loss_fn_name == \"ce\":\n",
        "                    loss = criterion(logits, batch['labels'])\n",
        "                else:\n",
        "                    loss = focal_loss(logits, batch['labels'], gamma=2.0, class_weights=class_w)\n",
        "\n",
        "                accelerator.backward(loss)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "                total_train_loss += loss.item()\n",
        "                batch_bar.set_postfix(loss=float(loss))\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            all_preds, all_labels = [], []\n",
        "            for batch in val_loader:\n",
        "                with torch.no_grad():\n",
        "                    logits = model(input_ids=batch['input_ids'],\n",
        "                                   attention_mask=batch['attention_mask'],\n",
        "                                   token_type_ids=batch.get('token_type_ids', None)).logits\n",
        "                    preds  = torch.argmax(logits, dim=1)\n",
        "                all_preds.extend(accelerator.gather(preds).cpu().numpy().tolist())\n",
        "                all_labels.extend(accelerator.gather(batch['labels']).cpu().numpy().tolist())\n",
        "\n",
        "            val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "            print(f\"üìà Fold {fold}  Epoch {epoch}: F1={val_f1:.4f}\")\n",
        "            epoch_bar.set_postfix(F1=val_f1)\n",
        "\n",
        "            if val_f1 > best_f1_fold:\n",
        "                best_f1_fold, epochs_no_improve = val_f1, 0\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "            if epochs_no_improve >= 2:\n",
        "                break\n",
        "\n",
        "        print(f\"üü© Fold {fold}  Best F1: {best_f1_fold:.4f}\")\n",
        "        total_f1 += best_f1_fold\n",
        "\n",
        "        accelerator.free_memory()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Average Macro-F1 on 5 folds\n",
        "    return total_f1 / 5.0\n",
        "\n",
        "# Optuna\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
        "\n",
        "print(\"Best hyperparameters:\", study.best_trial.params)\n",
        "print(f\"Best average F1 on validation: {study.best_value:.4f}\")"
      ],
      "metadata": {
        "id": "9ndNVWXvJmzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, numpy as np, optuna\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from accelerate import Accelerator\n",
        "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def focal_loss(logits, targets, gamma=2.0, class_weights=None):\n",
        "    log_probs = nn.functional.log_softmax(logits.float(), dim=1)\n",
        "    probs = torch.exp(log_probs)\n",
        "    pt = probs[range(len(targets)), targets]\n",
        "    log_pt = log_probs[range(len(targets)), targets]\n",
        "    if class_weights is not None:\n",
        "        alpha = class_weights[targets]\n",
        "    else:\n",
        "        alpha = torch.ones_like(pt)\n",
        "    loss = -alpha * ((1 - pt) ** gamma) * log_pt\n",
        "    return loss.mean()\n",
        "\n",
        "best_lr = 2.980830878730812e-05\n",
        "best_batch = 16\n",
        "best_weight_decay = 0.07233960382914938\n",
        "best_epochs = 5\n",
        "best_warmup_ratio = 0.1\n",
        "best_loss_fn = 'focal'\n",
        "best_balance = 'sampler'\n",
        "\n",
        "# –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—Å–µ–π –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ —Å –ª—É—á—à–∏–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
        "final_model = BertForSequenceClassification.from_pretrained('ai-forever/ruBert-large', num_labels=3)\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "param_groups = [\n",
        "    {\"params\": [p for n, p in final_model.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": best_weight_decay},\n",
        "    {\"params\": [p for n, p in final_model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
        "]\n",
        "optimizer = AdamW(param_groups, lr=best_lr)\n",
        "\n",
        "# –î–∞—Ç–∞–ª–æ–∞–¥–µ—Ä –¥–ª—è –ø–æ–ª–Ω–æ–π –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏\n",
        "if best_balance == \"sampler\":\n",
        "    # WeightedRandomSampler –Ω–∞ –≤—Å–µ–π –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ\n",
        "    full_train_labels = train_df['sentiment_label'].values\n",
        "    class_counts = np.bincount(full_train_labels, minlength=3)\n",
        "    class_weights_for_sampler = 1.0 / class_counts\n",
        "    sample_weights = [class_weights_for_sampler[label] for label in full_train_labels]\n",
        "    sampler = torch.utils.data.WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "    full_train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=best_batch, sampler=sampler, collate_fn=collator)\n",
        "else:\n",
        "    full_train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=best_batch, shuffle=True, collate_fn=collator)\n",
        "\n",
        "# –°—á–∏—Ç–∞–µ–º —à–∞–≥–∏ –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º scheduler\n",
        "num_steps_per_epoch = len(full_train_loader)\n",
        "total_steps = best_epochs * num_steps_per_epoch\n",
        "warmup_steps = int(best_warmup_ratio * total_steps)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
        "\n",
        "# Accelerator –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
        "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
        "final_model, optimizer, full_train_loader, scheduler = accelerator.prepare(final_model, optimizer, full_train_loader, scheduler)\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
        "if best_loss_fn == \"ce\":\n",
        "    if best_balance == \"class_weights\":\n",
        "        class_counts = np.bincount(train_df['label'].values, minlength=3)\n",
        "        class_weights = (len(train_df) / class_counts) / 3.0\n",
        "        class_weights = torch.tensor(class_weights, dtype=torch.float).to(accelerator.device)\n",
        "        final_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    else:\n",
        "        final_criterion = nn.CrossEntropyLoss()\n",
        "else:  # focal\n",
        "    if best_balance == \"class_weights\":\n",
        "        class_counts = np.bincount(train_df['label'].values, minlength=3)\n",
        "        class_weights = (len(train_df) / class_counts) / 3.0\n",
        "        class_weights = torch.tensor(class_weights, dtype=torch.float).to(accelerator.device)\n",
        "    else:\n",
        "        class_weights = None\n",
        "    final_criterion = None  # –±—É–¥–µ–º –≤—ã–∑—ã–≤–∞—Ç—å focal_loss –≤—Ä—É—á–Ω—É—é\n",
        "\n",
        "final_model.train()\n",
        "for epoch in tqdm(range(1, best_epochs + 1), desc=\"–≠–ø–æ—Ö–∏\"):\n",
        "    epoch_loss = 0.0\n",
        "    batch_bar = tqdm(full_train_loader, desc=f\"–≠–ø–æ—Ö–∞ {epoch}\", leave=False)\n",
        "\n",
        "    for batch in batch_bar:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = final_model(\n",
        "            input_ids=batch['input_ids'],\n",
        "            attention_mask=batch['attention_mask'],\n",
        "            token_type_ids=batch.get('token_type_ids', None),\n",
        "            labels=None\n",
        "        )\n",
        "        logits = outputs.logits\n",
        "\n",
        "        if best_loss_fn == \"ce\":\n",
        "            loss = final_criterion(logits, batch['labels'])\n",
        "        else:\n",
        "            loss = focal_loss(logits, batch['labels'], gamma=2.0, class_weights=class_weights)\n",
        "\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        batch_bar.set_postfix(loss=float(loss))\n",
        "\n",
        "    tqdm.write(f\"üìä –≠–ø–æ—Ö–∞ {epoch}/{best_epochs} ‚Äî –°—Ä–µ–¥–Ω–∏–π loss: {epoch_loss/len(full_train_loader):.4f}\")\n",
        "\n",
        "# –û—Ç–∫–ª—é—á–∞–µ–º —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è –ø–µ—Ä–µ–¥ –æ—Ü–µ–Ω–∫–æ–π\n",
        "final_model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "final_model.to(device)\n",
        "\n",
        "# –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π (holdout) –≤—ã–±–æ—Ä–∫–µ\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=best_batch, shuffle=False, collate_fn=collator)\n",
        "\n",
        "# all_preds = []\n",
        "# all_probs = []\n",
        "# all_true = []\n",
        "\n",
        "# for batch in test_loader:\n",
        "#     with torch.no_grad():\n",
        "#         outputs = final_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], token_type_ids=batch.get('token_type_ids', None))\n",
        "#         logits = outputs.logits\n",
        "#         probs = nn.functional.softmax(logits, dim=1)\n",
        "#         preds = torch.argmax(probs, dim=1)\n",
        "#     # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–Ω–∞ CPU –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫)\n",
        "#     all_preds.extend(preds.cpu().numpy().tolist())\n",
        "#     all_probs.extend(probs.cpu().numpy().tolist())\n",
        "#     all_true.extend(batch['labels'].cpu().numpy().tolist())\n",
        "\n",
        "all_preds = []\n",
        "all_probs = []\n",
        "all_true = []\n",
        "\n",
        "for batch in test_loader:\n",
        "    # –í–ê–ñ–ù–û: –ø–µ—Ä–µ–Ω–æ—Å–∏–º –≤—Ö–æ–¥–Ω–æ–π –±–∞—Ç—á –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = final_model(\n",
        "            input_ids=batch['input_ids'],\n",
        "            attention_mask=batch['attention_mask'],\n",
        "            token_type_ids=batch.get('token_type_ids', None)\n",
        "        )\n",
        "        logits = outputs.logits\n",
        "        probs = nn.functional.softmax(logits, dim=1)\n",
        "        preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "    # –ü–ï–†–ï–ù–û–° –ù–ê CPU –î–õ–Ø sklearn\n",
        "    all_preds.extend(preds.cpu().numpy().tolist())\n",
        "    all_probs.extend(probs.cpu().numpy().tolist())\n",
        "    all_true.extend(batch['labels'].cpu().numpy().tolist())\n",
        "\n",
        "\n",
        "# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy –º–∞—Å—Å–∏–≤—ã\n",
        "all_preds = np.array(all_preds)\n",
        "all_probs = np.array(all_probs)\n",
        "all_true = np.array(all_true)\n",
        "\n",
        "# –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫\n",
        "accuracy = accuracy_score(all_true, all_preds)\n",
        "macro_f1 = f1_score(all_true, all_preds, average='macro')\n",
        "macro_roc_auc = roc_auc_score(all_true, all_probs, average='macro', multi_class='ovr')\n",
        "cm = confusion_matrix(all_true, all_preds)\n",
        "\n",
        "print(f\"\\n–ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ:\")\n",
        "print(f\"Accuracy = {accuracy:.4f}\")\n",
        "print(f\"Macro F1 = {macro_f1:.4f}\")\n",
        "print(f\"Macro ROC-AUC = {macro_roc_auc:.4f}\")\n",
        "\n",
        "# –í—ã–≤–æ–¥ confusion matrix\n",
        "labels_name = {0: \"neg\", 1: \"neu\", 2: \"pos\"}\n",
        "cm_df = pd.DataFrame(cm, index=[labels_name[i] for i in range(3)], columns=[labels_name[i] for i in range(3)])\n",
        "print(\"\\nConfusion Matrix (actual x predicted):\")\n",
        "print(cm_df)"
      ],
      "metadata": {
        "id": "VFztHJQx8wtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **–ê–Ω–∞–ª–∏–∑ –æ—Ç–∑—ã–≤–æ–≤: —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–æ–¥–µ–ª—å —Å HDBSCAN –∏ c-TF-IDF**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-BR7FHk_1txu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã –ø—Ä–æ–≤–µ–¥—ë–º —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –æ—Ç–∑—ã–≤–æ–≤ –æ –∂–∏–ª—ã—Ö –∫–æ–º–ø–ª–µ–∫—Å–∞—Ö. –î–ª—è —ç—Ç–æ–≥–æ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (Sentence Transformers) –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –æ—Ç–∑—ã–≤–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º –≤–∏–¥–µ, –º–µ—Ç–æ–¥ —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ UMAP –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é HDBSCAN –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–¥–µ–ª–µ–Ω–∏—è –≥—Ä—É–ø–ø —Å—Ö–æ–∂–∏—Ö –æ—Ç–∑—ã–≤–æ–≤. –ü–æ—Å–ª–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã (—Ç–µ–º—ã) –≤—ã—á–∏—Å–ª–∏–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å –ø–æ–º–æ—â—å—é c-TF-IDF (class-based TF-IDF), —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∫–∞–∫–∏–µ —Å–ª–æ–≤–∞ –Ω–∞–∏–±–æ–ª–µ–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã –¥–ª—è –¥–∞–Ω–Ω–æ–π —Ç–µ–º—ã. –°–º–æ—Ç—Ä–µ—Ç—å –æ—Ç–∑—ã–≤—ã –±—É–¥–µ–º –≤ —Ä–∞–∑—Ä–µ–∑–µ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö (sentiment_label = 0), –∞ —Ç–∞–∫–∂–µ –Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö (sentiment_label = 1/2)"
      ],
      "metadata": {
        "id": "RlbBZbbp18Sl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö**"
      ],
      "metadata": {
        "id": "tU2oPcJe2Trd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('reviews_with_sentiment.csv')\n",
        "print(\"–í—Å–µ–≥–æ –æ—Ç–∑—ã–≤–æ–≤:\", len(df))\n",
        "print(\"–°—Ç–æ–ª–±—Ü—ã:\", df.columns.tolist())\n",
        "print(\"–ü—Ä–∏–º–µ—Ä –∑–∞–ø–∏—Å–µ–π:\")\n",
        "print(df[['doc_id', 'developer', 'rating', 'full_review_text', 'sentiment_label']])"
      ],
      "metadata": {
        "id": "u2RZcI621jdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ö–∞–∂–¥–∞—è –∑–∞–ø–∏—Å—å —Å–æ–¥–µ—Ä–∂–∏—Ç **—Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞ (full_review_text)**, **–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞ (doc_id)**, **–∏–º—è –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–∞ (developer)**, **–æ—Ü–µ–Ω–∫—É (rating)** –∏ **–º–µ—Ç–∫—É —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ sentiment_label (–≥–¥–µ 0 ‚Äì –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π –æ—Ç–∑—ã–≤, 1 ‚Äì –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π, 2 ‚Äì –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π)**. –ù–∏–∂–µ —Ä–∞–∑–¥–µ–ª–∏–º –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ –¥–≤–µ —á–∞—Å—Ç–∏ –ø–æ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏:"
      ],
      "metadata": {
        "id": "FirpeEnb2y9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "negative_reviews = df[df['sentiment_label'] == 0]\n",
        "non_negative_reviews = df[df['sentiment_label'].isin([1, 2])]\n",
        "\n",
        "print(\"–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤:\", len(negative_reviews))\n",
        "print(\"–ù–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤:\", len(non_negative_reviews))"
      ],
      "metadata": {
        "id": "6FkjmL2e2bru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –æ—Ç–∑—ã–≤–æ–≤ –º–æ–¥–µ–ª—å—é Sentence Transformer**"
      ],
      "metadata": {
        "id": "f3VQbHdB3FNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "–î–ª—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç—ã –æ—Ç–∑—ã–≤–æ–≤ –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ ‚Äì —á–∏—Å–ª–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–∞, –æ—Ç—Ä–∞–∂–∞—é—â–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞. –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—É—é **–º–æ–¥–µ–ª—å SentenceTransformer**, —Å–ø–æ—Å–æ–±–Ω—É—é –ø—Ä–µ–≤—Ä–∞—â–∞—Ç—å —Ñ—Ä–∞–∑—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ."
      ],
      "metadata": {
        "id": "UVSA31oC3Ta6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers umap-learn hdbscan"
      ],
      "metadata": {
        "id": "rCjtxkU63B8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')"
      ],
      "metadata": {
        "id": "jm1VxNS-3Z5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–¢–µ–ø–µ—Ä—å –ø–æ–ª—É—á–∏–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –æ—Ç–∑—ã–≤–æ–≤. –ß—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, —Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º —Å—Ä–∞–∑—É –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –æ—Ç–∑—ã–≤–æ–≤"
      ],
      "metadata": {
        "id": "JHBT9Hq54KY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_reviews = df['full_review_text'].astype(str).tolist()\n",
        "embeddings = model.encode(all_reviews, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "dszz3v8e3xkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–í–æ—Ç –∫–∞–∫ –≤—ã–≥–ª—è–¥–∏—Ç –æ—Ç–∑—ã–≤ –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ –≤–µ–∫—Ç–æ—Ä (—ç–º–±–µ–¥–¥–∏–Ω–≥)"
      ],
      "metadata": {
        "id": "52lrEYEc4Uft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –æ–¥–Ω–æ–≥–æ –æ—Ç–∑—ã–≤–∞:\", embeddings.shape[1])\n",
        "print(\"–û—Ç–∑—ã–≤:\\n\", all_reviews[0])\n",
        "print(\"–≠–º–±–µ–¥–¥–∏–Ω–≥ –æ—Ç–∑—ã–≤–∞:\\n\", embeddings[0][:5], \"...\")"
      ],
      "metadata": {
        "id": "1VDnaP684Til"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ú–æ–¥–µ–ª—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–ª–∞ –∫–∞–∂–¥—ã–π –æ—Ç–∑—ã–≤ –≤ 384-–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä. –≠—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –±—É–¥—É—Ç –æ—Å–Ω–æ–≤–æ–π –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ ‚Äì –æ—Ç–∑—ã–≤—ã —Å –ø–æ—Ö–æ–∂–∏–º —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ–º –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å —Å—Ö–æ–∂–∏–µ –≤–µ–∫—Ç–æ—Ä—ã."
      ],
      "metadata": {
        "id": "3tOTTh3C48sn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –æ—Ç–∑—ã–≤–æ–≤ HDBSCAN/K-means+ –≤—ã–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–º —á–µ—Ä–µ–∑ c-TF-IDF**"
      ],
      "metadata": {
        "id": "K4Pipl7_5Bkj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "–î–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º HDBSCAN (Hierarchical DBSCAN), –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–ª–æ—Ç–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã —Ä–∞–∑–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –∏ –ø–æ–º–µ—á–∞–µ—Ç —Ç–æ—á–∫–∏, –Ω–µ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—â–∏–µ –Ω–∏ –æ–¥–Ω–æ–º—É –∫–ª–∞—Å—Ç–µ—Ä—É, –∫–∞–∫ —à—É–º (outliers). –í–∞–∂–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä ‚Äì min_cluster_size=15, —Ç–æ –µ—Å—Ç—å —Ç–µ–º–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–∏–Ω–∏–º—É–º 15 –æ—Ç–∑—ã–≤–æ–≤, –∏–Ω–∞—á–µ –æ—Ç–∑—ã–≤—ã —Å—á–∏—Ç–∞—é—Ç—Å—è –≤—ã–±—Ä–æ—Å–∞–º–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç—Å–µ—è—Ç—å –µ–¥–∏–Ω–∏—á–Ω—ã–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –æ—Ç–∑—ã–≤—ã, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ç–µ–º—ã."
      ],
      "metadata": {
        "id": "m3ti8x0K6uUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ü–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –æ–ø—Ä–µ–¥–µ–ª–∏–º —Ç–µ–º—ã ‚Äì —Ç.–µ. –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ ‚Äì —Å –ø–æ–º–æ—â—å—é c-TF-IDF. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –≤—Å–µ —Ç–µ–∫—Å—Ç—ã –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ –≤ –æ–¥–∏–Ω \"–¥–æ–∫—É–º–µ–Ω—Ç\" –∏ –≤—ã—á–∏—Å–ª—è–µ—Ç TF-IDF –¥–ª—è —ç—Ç–æ–≥–æ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –≤—Å–µ—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –ø–æ–ª—É—á–∞–µ–º —Å–ª–æ–≤–∞, –Ω–∞–∏–±–æ–ª–µ–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ (—á–∞—Å—Ç—ã–µ –≤ –Ω—ë–º –∏ —Ä–µ–¥–∫–∏–µ –≤ –¥—Ä—É–≥–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö)–∞ –∑–∞—Ç–µ–º –Ω–∞—Ö–æ–¥–∏–º –∏—Ö –≤–µ—Å–∞ –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ø-10 –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞."
      ],
      "metadata": {
        "id": "2N6KbvLS6z8U"
      }
    }
  ]
}
