{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Для классификации тональности русскоязычных отзывов о застройщиках  будем использовать **предобученную языковую модель ruBERT-large с Hugging Face от SberDevices**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "K6ARBrydipsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **План обучения выглядит так:**\n",
        "\n",
        "1.   Загрузка и подготовка данных (определение целевой переменной, разметка данных)\n",
        "2.   Предобработка текста\n",
        "3.   Настройка модели\n",
        "4.   Подбор гиперпараметров через кросс-валидацию с Optuna\n",
        "5.   Оценка качетсва модели на размеченных данных\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9xyB80BKjyqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Загрузка и подготовка данных**"
      ],
      "metadata": {
        "id": "6W2FPoqDlMCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для определения целевой переменной (тональности) воспользуемся рейтингом отзыва - поле ***rating***. Поле ***rating*** принимает значения от **1** до **5** (звездочки в отзыве). Мы переведем их в метки тональности: отзывы с рейтингом **1** или **2** считаются **негативными** (*-1*, закодируем как класс *0*), рейтинг **3** — **нейтральный** (*0*, класс *1*), а **4** или **5** — **позитивный** (*+1*, класс *2*)."
      ],
      "metadata": {
        "id": "5-iHbpb3lkD2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QokTZcj8iGzF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('all_reviews_bert.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Map ratings (1-5) to sentiment labels (0=negative, 1=neutral, 2=positive)\n",
        "def rating_to_label(rating):\n",
        "    if rating <= 2.0:\n",
        "        return 0\n",
        "    elif rating == 3.0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 2\n",
        "\n",
        "df['sentiment_label'] = df['rating'].apply(rating_to_label).astype(int)"
      ],
      "metadata": {
        "id": "NaJzd19PbX7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment class distribution\n",
        "class_counts = df['sentiment_label'].value_counts()\n",
        "print(\"Class distribution:\")\n",
        "for label, count in class_counts.items():\n",
        "    print(f\"{label}: {count}\")"
      ],
      "metadata": {
        "id": "IgcGwvdgqyeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Видим, что классы распределены неравномерно - наблюдается явный дисбаланс.\n",
        "\n",
        "Для надёжной оценки качества модели будем использовать стратифицированную разбивку данных на тренировочную и тестовую выборки, сохраняя пропорции классов. Отложим **20% данных** в качестве тестовой выборки (holdout) для финальной оценки, а оставшиеся 80% используем для обучения и подбора гиперпараметров.\n"
      ],
      "metadata": {
        "id": "XkXaf96jrjr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting dataset on training and test sample (startified by sentiment)\n",
        "train_df, test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    stratify=df['sentiment_label'],\n",
        "    random_state=42)\n",
        "\n",
        "print(f\"Train sample size: {len(train_df)}, Test sample size: {len(test_df)}\")\n",
        "\n",
        "# Reseting index\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "ita8Iu9Iq1Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Предобработка текста с помощью Natasha**"
      ],
      "metadata": {
        "id": "gA62rhLfyhGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перед обучением модели выполним предобработку текста: очистим от лишних символов и приведем слова к нормальной форме (лемматизация). Для русского языка мы используем библиотеку **Natasha**, которая обеспечивает качественную токенизацию и лемматизацию с учетом морфологии.\n",
        "\n",
        "Шаги предобработки для каждого отзыва:\n",
        "*   Токенизация текста (разбиение на слова и знаки препинания)\n",
        "*   Удаление или игнорирование пунктуации, чисел и лишних символов\n",
        "*   Приведение слов к начальной форме (лемматизация) с помощью морфологического анализатора Natasha\n",
        "*   Приведение текста к нижнему регистру\n",
        "\n",
        "\n",
        "После этих шагов текст станет более стандартизированным, что облегчает обучение модели (модель не будет учитывать различия в формах слов, а лишь их основы). Ниже определим функцию **preprocess_text** для этой задачи. Затем применим её ко всем отзывам в обучающей и тестовых выборках."
      ],
      "metadata": {
        "id": "0y1AB10KynF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install natasha"
      ],
      "metadata": {
        "id": "kbGCQpyPwDeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from natasha import Segmenter, NewsEmbedding, NewsMorphTagger, MorphVocab, Doc\n",
        "import re\n",
        "\n",
        "segmenter = Segmenter()\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "morph_vocab = MorphVocab()\n",
        "\n",
        "def lemmatize_text(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^а-яёА-ЯЁa-zA-Z\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    doc = Doc(text)\n",
        "    doc.segment(segmenter)\n",
        "    doc.tag_morph(morph_tagger)\n",
        "    for token in doc.tokens:\n",
        "        token.lemmatize(morph_vocab)\n",
        "\n",
        "    lemmas = \" \".join([token.lemma for token in doc.tokens if token.lemma])\n",
        "    return lemmas\n",
        "\n",
        "train_df['text_processed'] = train_df['full_review_text'].apply(lemmatize_text)\n",
        "test_df['text_processed']  = test_df['full_review_text'].apply(lemmatize_text)"
      ],
      "metadata": {
        "id": "D473mRW2zQBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример: посмотрим оригинальный и обработанный текст для пары отзывов"
      ],
      "metadata": {
        "id": "y3pcnz320Oox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_indices = [0, 1]\n",
        "for idx in example_indices:\n",
        "    orig_text = train_df.loc[idx, 'full_review_text']\n",
        "    proc_text = train_df.loc[idx, 'text_processed']\n",
        "    print(f\"Example {idx}:\")\n",
        "    print(\"Original text:\", orig_text)\n",
        "    print(\"After lemmatization:\", proc_text)"
      ],
      "metadata": {
        "id": "2OvI-Vcz0Pk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. **Токенизация и формирование наборов данных для модели BERT**"
      ],
      "metadata": {
        "id": "fEpJVlfp1lyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для обучения модели BERT нам нужно преобразовать тексты в числовой формат – токенизировать их с помощью соответствующего токенизатора. Используем предобученный токенизатор от модели ai-forever/ruBert-large из библиотеки 🤗 Transformers. Он приведет каждое слово (или часть слова) к идентификатору из словаря модели.\n",
        "\n",
        "- Зададим максимальную длину последовательности MAX_LEN = 256 токенов. Если отзыв длиннее, он будет усечен до 256 токенов, если короче – будет дополнен специальными токенами заполнителями при формировании батча.\n",
        "- Чтобы эффективно обрабатывать данные разной длины, воспользуемся DataCollatorWithPadding. Он будет автоматически паддировать (дополнять) меньшие последовательности до длины самой длинной в текущем батче. Так мы не расходуем лишнюю память на глобальное дополнение всех последовательностей до 256, а только внутри батча.\n",
        "- Подготовим кастомный класс Dataset для наших данных, который по индексу будет возвращать токенизированный отзыв и соответствующую метку. Это позволит легко использовать DataLoader для итерации по данным батчами."
      ],
      "metadata": {
        "id": "8eBv-9u61wpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Initializing ruBERT-large tokenizator\n",
        "model_name = \"ai-forever/ruBert-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "token_max_len = 256\n",
        "\n",
        "# Tokenizing training and test sample texts\n",
        "train_encodings = tokenizer(\n",
        "    list(train_df['text_processed']),\n",
        "    truncation=True,\n",
        "    padding=False,\n",
        "    max_length=token_max_len\n",
        ")\n",
        "\n",
        "test_encodings = tokenizer(\n",
        "    list(test_df['text_processed']),\n",
        "    truncation=True,\n",
        "    padding=False,\n",
        "    max_length=token_max_len\n",
        ")\n",
        "\n",
        "# Prepare torch Dataset objects\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        # vocabulary with keys 'input_ids', 'attention_mask'\n",
        "        self.encodings = encodings\n",
        "        # labels list\n",
        "        self.labels = labels\n",
        "    def __getitem__(self, idx):\n",
        "        # Taking tokenized representations for this index\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        # Adding labels\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Creating Dataset objects for training and test samples\n",
        "train_dataset = ReviewDataset(train_encodings, train_df['sentiment_label'].tolist())\n",
        "test_dataset = ReviewDataset(test_encodings, test_df['sentiment_label'].tolist())\n",
        "\n",
        "# Creating collator for dynamic padding batches\n",
        "collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "eO-6hk3g0UM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример: продемонстрируем работу токенизатора на обработанном тексте"
      ],
      "metadata": {
        "id": "OGwpSmKK4xTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = train_df.loc[0, 'text_processed']\n",
        "tokens = tokenizer.tokenize(sample_text)\n",
        "input_ids = train_encodings['input_ids'][0]\n",
        "\n",
        "print(\"Lemmatized text:\")\n",
        "print(sample_text)\n",
        "print(\"\\nBERT tokens:\")\n",
        "print(tokens)\n",
        "print(\"\\nTokens IDs:\")\n",
        "print(input_ids)"
      ],
      "metadata": {
        "id": "fKDfPNaD4nRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Настройка обучения: модель, оптимизатор и гиперпараметры**"
      ],
      "metadata": {
        "id": "ap2YHCDR45mT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы используем предобученную модель **BertForSequenceClassification** (на основе ruBERT-large) с выходным слоем классификации на 3 класса. Модель добавляет над [CLS]-представлением линейный классификатор и softmax для предсказания вероятностей классов.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#### **План обучения:**\n",
        "\n",
        "- **Модель:** BertForSequenceClassification.from_pretrained('ai-forever/ruBert-large', num_labels=3).\n",
        "- **Оптимизатор:** AdamW (адам с декей на весах) – стандарт для BERT. Мы настроим weight decay для всех весов кроме коэффициентов смещения и LayerNorm (их обычно не подвергают L2-регуляризации).\n",
        "- **Scheduler:** линейный планировщик обучения с прогревом (get_linear_schedule_with_warmup) – сначала небольшое warmup_ratio эпох итераций для разогрева (низкий learning rate), затем линейное уменьшение learning rate до 0 к концу обучения.\n",
        "- **Loss-функция:** будем экспериментировать с обычной кросс-энтропией (CrossEntropyLoss) и Focal Loss, которая может улучшить обучение на несбалансированных данных, фокусируясь на трудноклассифицируемых примерах. Также рассмотрим два подхода к учёту дисбаланса:\n",
        "    - Взвешивание классов (class weights) в функции потерь.\n",
        "    - WeightedRandomSampler – балансировка выборки путём более частого выбора редких классов при формировании батчей.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "sXMXSV2E4_bz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Нам предстоит подобрать ряд гиперпараметров:**\n",
        "- ***learning_rate*** – скорость обучения (от 1e-5 до 5e-5, логарифмический масштаб).\n",
        "- ***batch_size*** – размер батча (8, 16 или 32).\n",
        "- ***weight_decay*** – коэффициент L2-регуляризации (0.0 до 0.1).\n",
        "- ***epochs*** – максимальное число эпох обучения (от 2 до 6).\n",
        "- ***warmup_ratio*** – доля разогрева (0.0, 0.1 или 0.2 от всего числа шагов).\n",
        "- ***loss_fn*** – тип функции потерь: ce (CrossEntropy) или focal (FocalLoss).\n",
        "- ***balance_strategy*** – стратегия борьбы с дисбалансом: class_weights или sampler."
      ],
      "metadata": {
        "id": "wAmoOE756MXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы найти оптимальные значения этих гиперпараметров, мы применим поиск с помощью **Optuna**. В качестве целевой метрики будем максимизировать **Macro F1** на валидации. Мы разделим обучающие данные на 5 фолдов (Stratified K-Fold) и будем в рамках каждого набора гиперпараметров обучать модель на 4 фолдах и оценивать Macro F1 на оставшемся фолде, усредняя результаты по 5 фолдам. Таким образом, Optuna будет оценивать качество каждого набора гиперпараметров на основе кросс-валидации\n",
        "\n",
        "Во время обучения на каждом фолде реализуем Early Stopping: если Macro F1 на валидации не улучшается в течение 2 эпох подряд, останавливаем обучение на этом фолде досрочно, чтобы не переобучаться и сэкономить время. Будем сохранять наилучшее значение F1 для каждого фолда.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0bznkKhf6vRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Настройка Optuna и кросс-валидации**\n",
        "\n",
        "Определим функцию-цель для Optuna, которая принимает набор гиперпараметров, обучает модель на 5-фолдовой кросс-валидации и возвращает средний Macro F1. Внутри нее происходит следующее:\n",
        "\n",
        "- Инициализация модели и оптимизатора для каждого фолда с заданными гиперпараметрами.\n",
        "- Применение выбранной стратегии балансировки: либо установка весов классов в функции потерь, либо использование WeightedRandomSampler для генерации батчей.\n",
        "- Вычисление количества тренировочных шагов и настройка linear scheduler с разогревом.\n",
        "- Цикл обучения по эпохам с отслеживанием best F1 на валидации и условием ранней остановки.\n",
        "- После обучения на всех фолдах – возврат среднего Macro F1 для данного набора гиперпараметров."
      ],
      "metadata": {
        "id": "Mhqa5eJ473Vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "31uvHCyb46xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "4vBOcPA0_tLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "Xxa31tcyBbl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, numpy as np, optuna\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from accelerate import Accelerator\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 1. Focal-loss\n",
        "def focal_loss(logits, targets, gamma=2.0, class_weights=None):\n",
        "    log_probs = nn.functional.log_softmax(logits.float(), dim=1)\n",
        "    probs     = torch.exp(log_probs)\n",
        "    pt        = probs[range(len(targets)), targets]\n",
        "    log_pt    = log_probs[range(len(targets)), targets]\n",
        "    alpha     = class_weights[targets] if class_weights is not None else torch.ones_like(pt)\n",
        "    return (-alpha * ((1 - pt) ** gamma) * log_pt).mean()\n",
        "\n",
        "# 2. Stratified 5-fold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# array of labels\n",
        "y_all = train_df['sentiment_label'].values\n",
        "\n",
        "# 3. Optuna objective\n",
        "def objective(trial):\n",
        "    learning_rate  = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
        "    batch_size     = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
        "    weight_decay   = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n",
        "    epochs         = trial.suggest_int(\"epochs\", 2, 6)\n",
        "    warmup_ratio   = trial.suggest_categorical(\"warmup_ratio\", [0.0, 0.1, 0.2])\n",
        "    loss_fn_name   = trial.suggest_categorical(\"loss_fn\", [\"ce\", \"focal\"])\n",
        "    balance_strat  = trial.suggest_categorical(\"balance_strategy\", [\"class_weights\", \"sampler\"])\n",
        "\n",
        "    total_f1 = 0.0\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(y_all)), y_all), 1):\n",
        "\n",
        "        print(f\"Fold {fold}/5  —  trial #{trial.number}\")\n",
        "        train_subset = torch.utils.data.Subset(train_dataset, train_idx)\n",
        "        val_subset   = torch.utils.data.Subset(train_dataset,   val_idx)\n",
        "\n",
        "        # Balancing classes\n",
        "        if balance_strat == \"sampler\":\n",
        "            class_cnt  = np.bincount(y_all[train_idx], minlength=3)\n",
        "            w_cls      = 1.0 / class_cnt\n",
        "            sample_w   = [w_cls[y] for y in y_all[train_idx]]\n",
        "            sampler    = WeightedRandomSampler(sample_w, len(sample_w), replacement=True)\n",
        "            train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size,\n",
        "                                                       sampler=sampler, collate_fn=collator)\n",
        "        else:\n",
        "            train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size,\n",
        "                                                       shuffle=True,  collate_fn=collator)\n",
        "\n",
        "        val_loader   = torch.utils.data.DataLoader(val_subset,   batch_size=batch_size,\n",
        "                                                   shuffle=False, collate_fn=collator)\n",
        "\n",
        "        # Model and optimizator\n",
        "        model = BertForSequenceClassification.from_pretrained(\n",
        "            'ai-forever/ruBert-large',\n",
        "            num_labels=3\n",
        "        )\n",
        "\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        param_groups = [\n",
        "            {\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "             \"weight_decay\": weight_decay},\n",
        "            {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "             \"weight_decay\": 0.0}\n",
        "        ]\n",
        "\n",
        "        optimizer = AdamW(param_groups, lr=learning_rate)\n",
        "\n",
        "        steps_per_epoch   = len(train_loader)\n",
        "        total_train_steps = epochs * steps_per_epoch\n",
        "        warmup_steps      = int(warmup_ratio * total_train_steps)\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_train_steps)\n",
        "\n",
        "        accelerator = Accelerator(mixed_precision=\"fp16\")\n",
        "        model, optimizer, train_loader, val_loader, scheduler = accelerator.prepare(\n",
        "            model, optimizer, train_loader, val_loader, scheduler\n",
        "        )\n",
        "\n",
        "        # loss-function\n",
        "        if loss_fn_name == \"ce\":\n",
        "            if balance_strat == \"class_weights\":\n",
        "                cnt = np.bincount(y_all[train_idx], minlength=3)\n",
        "                w   = torch.tensor((len(train_idx)/cnt)/3.0, dtype=torch.float).to(accelerator.device)\n",
        "                criterion = nn.CrossEntropyLoss(weight=w)\n",
        "            else:\n",
        "                criterion = nn.CrossEntropyLoss()\n",
        "        else:\n",
        "            cnt = np.bincount(y_all[train_idx], minlength=3) if balance_strat==\"class_weights\" else None\n",
        "            class_w = torch.tensor((len(train_idx)/cnt)/3.0, dtype=torch.float).to(accelerator.device) if cnt is not None else None\n",
        "            criterion = None\n",
        "\n",
        "        # Epochs cycle with progress bar\n",
        "        best_f1_fold, epochs_no_improve = 0.0, 0\n",
        "        epoch_bar = tqdm(range(1, epochs+1), desc=f\"Epochs (fold {fold})\", leave=False)\n",
        "\n",
        "        for epoch in epoch_bar:\n",
        "            model.train()\n",
        "            batch_bar = tqdm(train_loader, desc=f\"Train ep{epoch}\", leave=False)\n",
        "\n",
        "            total_train_loss = 0.0\n",
        "            for batch in batch_bar:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(input_ids=batch['input_ids'],\n",
        "                                attention_mask=batch['attention_mask'],\n",
        "                                token_type_ids=batch.get('token_type_ids', None))\n",
        "                logits = outputs.logits\n",
        "\n",
        "                if loss_fn_name == \"ce\":\n",
        "                    loss = criterion(logits, batch['labels'])\n",
        "                else:\n",
        "                    loss = focal_loss(logits, batch['labels'], gamma=2.0, class_weights=class_w)\n",
        "\n",
        "                accelerator.backward(loss)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "                total_train_loss += loss.item()\n",
        "                batch_bar.set_postfix(loss=float(loss))\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            all_preds, all_labels = [], []\n",
        "            for batch in val_loader:\n",
        "                with torch.no_grad():\n",
        "                    logits = model(input_ids=batch['input_ids'],\n",
        "                                   attention_mask=batch['attention_mask'],\n",
        "                                   token_type_ids=batch.get('token_type_ids', None)).logits\n",
        "                    preds  = torch.argmax(logits, dim=1)\n",
        "                all_preds.extend(accelerator.gather(preds).cpu().numpy().tolist())\n",
        "                all_labels.extend(accelerator.gather(batch['labels']).cpu().numpy().tolist())\n",
        "\n",
        "            val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "            print(f\"📈 Fold {fold}  Epoch {epoch}: F1={val_f1:.4f}\")\n",
        "            epoch_bar.set_postfix(F1=val_f1)\n",
        "\n",
        "            if val_f1 > best_f1_fold:\n",
        "                best_f1_fold, epochs_no_improve = val_f1, 0\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "            if epochs_no_improve >= 2:\n",
        "                break\n",
        "\n",
        "        print(f\"🟩 Fold {fold}  Best F1: {best_f1_fold:.4f}\")\n",
        "        total_f1 += best_f1_fold\n",
        "\n",
        "        accelerator.free_memory()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Average Macro-F1 on 5 folds\n",
        "    return total_f1 / 5.0\n",
        "\n",
        "# Optuna\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
        "\n",
        "print(\"Best hyperparameters:\", study.best_trial.params)\n",
        "print(f\"Best average F1 on validation: {study.best_value:.4f}\")"
      ],
      "metadata": {
        "id": "9ndNVWXvJmzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, numpy as np, optuna\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from accelerate import Accelerator\n",
        "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def focal_loss(logits, targets, gamma=2.0, class_weights=None):\n",
        "    log_probs = nn.functional.log_softmax(logits.float(), dim=1)\n",
        "    probs = torch.exp(log_probs)\n",
        "    pt = probs[range(len(targets)), targets]\n",
        "    log_pt = log_probs[range(len(targets)), targets]\n",
        "    if class_weights is not None:\n",
        "        alpha = class_weights[targets]\n",
        "    else:\n",
        "        alpha = torch.ones_like(pt)\n",
        "    loss = -alpha * ((1 - pt) ** gamma) * log_pt\n",
        "    return loss.mean()\n",
        "\n",
        "best_lr = 2.980830878730812e-05\n",
        "best_batch = 16\n",
        "best_weight_decay = 0.07233960382914938\n",
        "best_epochs = 5\n",
        "best_warmup_ratio = 0.1\n",
        "best_loss_fn = 'focal'\n",
        "best_balance = 'sampler'\n",
        "\n",
        "# Обучение финальной модели на всей обучающей выборке с лучшими гиперпараметрами\n",
        "final_model = BertForSequenceClassification.from_pretrained('ai-forever/ruBert-large', num_labels=3)\n",
        "\n",
        "# Настройка оптимизатора и планировщика для финальной модели\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "param_groups = [\n",
        "    {\"params\": [p for n, p in final_model.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": best_weight_decay},\n",
        "    {\"params\": [p for n, p in final_model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
        "]\n",
        "optimizer = AdamW(param_groups, lr=best_lr)\n",
        "\n",
        "# Даталоадер для полной обучающей выборки\n",
        "if best_balance == \"sampler\":\n",
        "    # WeightedRandomSampler на всей обучающей выборке\n",
        "    full_train_labels = train_df['sentiment_label'].values\n",
        "    class_counts = np.bincount(full_train_labels, minlength=3)\n",
        "    class_weights_for_sampler = 1.0 / class_counts\n",
        "    sample_weights = [class_weights_for_sampler[label] for label in full_train_labels]\n",
        "    sampler = torch.utils.data.WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "    full_train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=best_batch, sampler=sampler, collate_fn=collator)\n",
        "else:\n",
        "    full_train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=best_batch, shuffle=True, collate_fn=collator)\n",
        "\n",
        "# Считаем шаги и настраиваем scheduler\n",
        "num_steps_per_epoch = len(full_train_loader)\n",
        "total_steps = best_epochs * num_steps_per_epoch\n",
        "warmup_steps = int(best_warmup_ratio * total_steps)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
        "\n",
        "# Accelerator для финального обучения\n",
        "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
        "final_model, optimizer, full_train_loader, scheduler = accelerator.prepare(final_model, optimizer, full_train_loader, scheduler)\n",
        "\n",
        "# Настройка функции потерь для финального обучения\n",
        "if best_loss_fn == \"ce\":\n",
        "    if best_balance == \"class_weights\":\n",
        "        class_counts = np.bincount(train_df['label'].values, minlength=3)\n",
        "        class_weights = (len(train_df) / class_counts) / 3.0\n",
        "        class_weights = torch.tensor(class_weights, dtype=torch.float).to(accelerator.device)\n",
        "        final_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    else:\n",
        "        final_criterion = nn.CrossEntropyLoss()\n",
        "else:  # focal\n",
        "    if best_balance == \"class_weights\":\n",
        "        class_counts = np.bincount(train_df['label'].values, minlength=3)\n",
        "        class_weights = (len(train_df) / class_counts) / 3.0\n",
        "        class_weights = torch.tensor(class_weights, dtype=torch.float).to(accelerator.device)\n",
        "    else:\n",
        "        class_weights = None\n",
        "    final_criterion = None  # будем вызывать focal_loss вручную\n",
        "\n",
        "final_model.train()\n",
        "for epoch in tqdm(range(1, best_epochs + 1), desc=\"Эпохи\"):\n",
        "    epoch_loss = 0.0\n",
        "    batch_bar = tqdm(full_train_loader, desc=f\"Эпоха {epoch}\", leave=False)\n",
        "\n",
        "    for batch in batch_bar:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = final_model(\n",
        "            input_ids=batch['input_ids'],\n",
        "            attention_mask=batch['attention_mask'],\n",
        "            token_type_ids=batch.get('token_type_ids', None),\n",
        "            labels=None\n",
        "        )\n",
        "        logits = outputs.logits\n",
        "\n",
        "        if best_loss_fn == \"ce\":\n",
        "            loss = final_criterion(logits, batch['labels'])\n",
        "        else:\n",
        "            loss = focal_loss(logits, batch['labels'], gamma=2.0, class_weights=class_weights)\n",
        "\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        batch_bar.set_postfix(loss=float(loss))\n",
        "\n",
        "    tqdm.write(f\"📊 Эпоха {epoch}/{best_epochs} — Средний loss: {epoch_loss/len(full_train_loader):.4f}\")\n",
        "\n",
        "# Отключаем режим обучения перед оценкой\n",
        "final_model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "final_model.to(device)\n",
        "\n",
        "# Оценка на тестовой (holdout) выборке\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=best_batch, shuffle=False, collate_fn=collator)\n",
        "\n",
        "# all_preds = []\n",
        "# all_probs = []\n",
        "# all_true = []\n",
        "\n",
        "# for batch in test_loader:\n",
        "#     with torch.no_grad():\n",
        "#         outputs = final_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], token_type_ids=batch.get('token_type_ids', None))\n",
        "#         logits = outputs.logits\n",
        "#         probs = nn.functional.softmax(logits, dim=1)\n",
        "#         preds = torch.argmax(probs, dim=1)\n",
        "#     # Собираем результаты (на CPU для вычисления метрик)\n",
        "#     all_preds.extend(preds.cpu().numpy().tolist())\n",
        "#     all_probs.extend(probs.cpu().numpy().tolist())\n",
        "#     all_true.extend(batch['labels'].cpu().numpy().tolist())\n",
        "\n",
        "all_preds = []\n",
        "all_probs = []\n",
        "all_true = []\n",
        "\n",
        "for batch in test_loader:\n",
        "    # ВАЖНО: переносим входной батч на устройство модели\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = final_model(\n",
        "            input_ids=batch['input_ids'],\n",
        "            attention_mask=batch['attention_mask'],\n",
        "            token_type_ids=batch.get('token_type_ids', None)\n",
        "        )\n",
        "        logits = outputs.logits\n",
        "        probs = nn.functional.softmax(logits, dim=1)\n",
        "        preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "    # ПЕРЕНОС НА CPU ДЛЯ sklearn\n",
        "    all_preds.extend(preds.cpu().numpy().tolist())\n",
        "    all_probs.extend(probs.cpu().numpy().tolist())\n",
        "    all_true.extend(batch['labels'].cpu().numpy().tolist())\n",
        "\n",
        "\n",
        "# Преобразуем в numpy массивы\n",
        "all_preds = np.array(all_preds)\n",
        "all_probs = np.array(all_probs)\n",
        "all_true = np.array(all_true)\n",
        "\n",
        "# Вычисление метрик\n",
        "accuracy = accuracy_score(all_true, all_preds)\n",
        "macro_f1 = f1_score(all_true, all_preds, average='macro')\n",
        "macro_roc_auc = roc_auc_score(all_true, all_probs, average='macro', multi_class='ovr')\n",
        "cm = confusion_matrix(all_true, all_preds)\n",
        "\n",
        "print(f\"\\nМетрики на тестовой выборке:\")\n",
        "print(f\"Accuracy = {accuracy:.4f}\")\n",
        "print(f\"Macro F1 = {macro_f1:.4f}\")\n",
        "print(f\"Macro ROC-AUC = {macro_roc_auc:.4f}\")\n",
        "\n",
        "# Вывод confusion matrix\n",
        "labels_name = {0: \"neg\", 1: \"neu\", 2: \"pos\"}\n",
        "cm_df = pd.DataFrame(cm, index=[labels_name[i] for i in range(3)], columns=[labels_name[i] for i in range(3)])\n",
        "print(\"\\nConfusion Matrix (actual x predicted):\")\n",
        "print(cm_df)"
      ],
      "metadata": {
        "id": "VFztHJQx8wtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Анализ отзывов: тематическая модель с HDBSCAN и c-TF-IDF**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-BR7FHk_1txu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В этом разделе мы проведём тематическое моделирование пользовательских отзывов о жилых комплексах. Для этого мы используем эмбеддинги предложений (Sentence Transformers) для представления отзывов в векторном виде, метод снижения размерности UMAP для визуализации и кластеризацию HDBSCAN для автоматического выделения групп схожих отзывов. После кластеризации для каждой группы (темы) вычислим ключевые слова с помощью c-TF-IDF (class-based TF-IDF), что позволяет определить, какие слова наиболее характерны для данной темы. Смотреть отзывы будем в разрезе отрицательных (sentiment_label = 0), а также неотрицательных (sentiment_label = 1/2)"
      ],
      "metadata": {
        "id": "RlbBZbbp18Sl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Загрузка и подготовка данных**"
      ],
      "metadata": {
        "id": "tU2oPcJe2Trd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('reviews_with_sentiment.csv')\n",
        "print(\"Всего отзывов:\", len(df))\n",
        "print(\"Столбцы:\", df.columns.tolist())\n",
        "print(\"Пример записей:\")\n",
        "print(df[['doc_id', 'developer', 'rating', 'full_review_text', 'sentiment_label']])"
      ],
      "metadata": {
        "id": "u2RZcI621jdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Каждая запись содержит **текст отзыва (full_review_text)**, **идентификатор документа (doc_id)**, **имя застройщика (developer)**, **оценку (rating)** и **метку тональности sentiment_label (где 0 – негативный отзыв, 1 – нейтральный, 2 – позитивный)**. Ниже разделим датасет на две части по тональности:"
      ],
      "metadata": {
        "id": "FirpeEnb2y9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "negative_reviews = df[df['sentiment_label'] == 0]\n",
        "non_negative_reviews = df[df['sentiment_label'].isin([1, 2])]\n",
        "\n",
        "print(\"Отрицательных отзывов:\", len(negative_reviews))\n",
        "print(\"Неотрицательных отзывов:\", len(non_negative_reviews))"
      ],
      "metadata": {
        "id": "6FkjmL2e2bru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. Векторизация отзывов моделью Sentence Transformer**"
      ],
      "metadata": {
        "id": "f3VQbHdB3FNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для тематического анализа преобразуем тексты отзывов в эмбеддинги – численные вектора, отражающие семантическое содержание текста. Мы используем предобученную многоязычную **модель SentenceTransformer**, способную превращать фразы на русском языке в векторное представление."
      ],
      "metadata": {
        "id": "UVSA31oC3Ta6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers umap-learn hdbscan"
      ],
      "metadata": {
        "id": "rCjtxkU63B8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')"
      ],
      "metadata": {
        "id": "jm1VxNS-3Z5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь получим эмбеддинги для отзывов. Чтобы избежать повторных вычислений, сформируем сразу векторные представления для всех отзывов"
      ],
      "metadata": {
        "id": "JHBT9Hq54KY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_reviews = df['full_review_text'].astype(str).tolist()\n",
        "embeddings = model.encode(all_reviews, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "dszz3v8e3xkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вот как выглядит отзыв после преобразования в вектор (эмбеддинг)"
      ],
      "metadata": {
        "id": "52lrEYEc4Uft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Размерность эмбеддинга одного отзыва:\", embeddings.shape[1])\n",
        "print(\"Отзыв:\\n\", all_reviews[0])\n",
        "print(\"Эмбеддинг отзыва:\\n\", embeddings[0][:5], \"...\")"
      ],
      "metadata": {
        "id": "1VDnaP684Til"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель преобразовала каждый отзыв в 384-мерный вектор. Эти эмбеддинги будут основой для кластеризации – отзывы с похожим содержанием должны иметь схожие векторы."
      ],
      "metadata": {
        "id": "3tOTTh3C48sn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. Кластеризация отзывов HDBSCAN/K-means+ выделение тем через c-TF-IDF**"
      ],
      "metadata": {
        "id": "K4Pipl7_5Bkj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для кластеризации используем алгоритм HDBSCAN (Hierarchical DBSCAN), который автоматически определяет плотные кластеры разного размера и помечает точки, не принадлежащие ни одному кластеру, как шум (outliers). Важный параметр – min_cluster_size=15, то есть тема должна содержать минимум 15 отзывов, иначе отзывы считаются выбросами. Это позволяет отсеять единичные уникальные отзывы, оставляя только повторяющиеся темы."
      ],
      "metadata": {
        "id": "m3ti8x0K6uUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "После получения кластеров определим темы – т.е. интерпретацию кластеров – с помощью c-TF-IDF. Этот метод комбинирует все тексты в кластере в один \"документ\" и вычисляет TF-IDF для этого агрегированного документа относительно всех кластеров, Таким образом, получаем слова, наиболее характерные для данного кластера (частые в нём и редкие в других кластерах)а затем находим их веса и извлекаем топ-10 важных слов каждого кластера."
      ],
      "metadata": {
        "id": "2N6KbvLS6z8U"
      }
    }
  ]
}
