{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Предобработка текста**"
      ],
      "metadata": {
        "id": "EHJKErqApDeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "s4eznPoxo7ES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('joined_with_onehot_to_use_final.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "dS7T2eWrpTg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int_cols = ['year_of_foundation', 'cnt_houses_built', 'years_on_market']\n",
        "for col in int_cols:\n",
        "    df[col] = df[col].astype(int)"
      ],
      "metadata": {
        "id": "WRVrt08kpWs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['sentiment_label'] != 1].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "PeY3mHuNp4L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Natasha"
      ],
      "metadata": {
        "id": "0gfsSJj_qFcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from natasha import Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger, Doc\n",
        "\n",
        "segmenter = Segmenter()\n",
        "morph_vocab = MorphVocab()\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "\n",
        "def lemmatize_text(text: str) -> str:\n",
        "    doc = Doc(text)\n",
        "    doc.segment(segmenter)\n",
        "    doc.tag_morph(morph_tagger)\n",
        "    for token in doc.tokens:\n",
        "        token.lemmatize(morph_vocab)\n",
        "\n",
        "    lemmas = [token.lemma for token in doc.tokens if token.lemma is not None and token.lemma.isalpha()]\n",
        "    return ' '.join(lemmas)"
      ],
      "metadata": {
        "id": "d3zOQr05qAoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['lemmatized_text'] = df['full_review_text'].astype(str).apply(lemmatize_text)"
      ],
      "metadata": {
        "id": "KEUhWpjdqPqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
        "text_embeddings = model.encode(df['lemmatized_text'].tolist(), show_progress_bar=True)\n",
        "text_embeddings = np.array(text_embeddings)"
      ],
      "metadata": {
        "id": "1gHvOzVqqRaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "numeric_features = ['rating', 'years_on_market', 'avg_sqm_price', 'cnt_houses_built']\n",
        "\n",
        "df['avg_sqm_price_log'] = np.log1p(df['avg_sqm_price'])\n",
        "df['cnt_houses_built_log'] = np.log1p(df['cnt_houses_built'])\n",
        "df['rating_log'] = df['rating']\n",
        "df['years_on_market_log'] = np.log1p(df['years_on_market'])\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df[['rating_norm', 'years_on_market_norm', 'avg_sqm_price_norm', 'cnt_houses_built_norm']] = scaler.fit_transform(\n",
        "    df[['rating_log', 'years_on_market_log', 'avg_sqm_price_log', 'cnt_houses_built_log']])"
      ],
      "metadata": {
        "id": "7A_5wYvErMDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Числовые признаки (нормализованные) с весом 0.3\n",
        "numeric_matrix = df[['rating_norm', 'years_on_market_norm', 'avg_sqm_price_norm', 'cnt_houses_built_norm']].values\n",
        "numeric_matrix_weighted = numeric_matrix * 0.3\n",
        "\n",
        "# Категориальные one-hot признаки с весом 0.1\n",
        "category_columns = [\"Эконом\", \"Комфорт\", \"Бизнес\", \"Премиум\"]\n",
        "if all(col in df.columns for col in category_columns):\n",
        "    category_matrix = df[category_columns].values.astype(float)\n",
        "else:\n",
        "    category_matrix = np.empty((len(df), 0))\n",
        "\n",
        "category_matrix_weighted = category_matrix * 0.1"
      ],
      "metadata": {
        "id": "3n7Grj1WrlVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Объединение эмбеддингов и всех признаков в одну матрицу признаков X_all\n",
        "X_all = np.hstack([text_embeddings, numeric_matrix_weighted, category_matrix_weighted])\n",
        "print(\"Размер матрицы признаков X_all:\", X_all.shape)"
      ],
      "metadata": {
        "id": "Tp1hphfrr-Du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Кластеризация отзывов K-Means/HDBSCAN**"
      ],
      "metadata": {
        "id": "lnjcN98ZsVnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import umap.umap_ as umap\n",
        "\n",
        "reducer_100 = umap.UMAP(n_components=100, metric='cosine', random_state=42)\n",
        "X_reduced_100 = reducer_100.fit_transform(X_all)"
      ],
      "metadata": {
        "id": "QEmUwOiWsEFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Определение оптимального числа кластеров (силуэт и локоть)\n",
        "possible_k = list(range(2, 16))\n",
        "silhouette_scores = []\n",
        "inertias = []\n",
        "X_for_clustering = X_reduced_100.copy()\n",
        "\n",
        "from sklearn.preprocessing import normalize\n",
        "X_for_clustering = normalize(X_for_clustering)\n",
        "\n",
        "for k in possible_k:\n",
        "    kmeans_temp = KMeans(n_clusters=k, init='k-means++', random_state=42, n_init='auto')\n",
        "    labels_temp = kmeans_temp.fit_predict(X_for_clustering)\n",
        "    inertias.append(kmeans_temp.inertia_)\n",
        "\n",
        "    sil_score = silhouette_score(X_for_clustering, labels_temp, metric='euclidean')\n",
        "    silhouette_scores.append(sil_score)\n",
        "    print(f\"K={k}: силуэт={sil_score:.3f}, инерция={kmeans_temp.inertia_:.0f}\")"
      ],
      "metadata": {
        "id": "sVIlWrgEspIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(possible_k, silhouette_scores, marker='o')\n",
        "plt.xlabel('Число кластеров K')\n",
        "plt.ylabel('Средний коэффициент силуэта')\n",
        "plt.title('Метод силуэта для выбора K')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(possible_k, inertias, marker='o', color='orange')\n",
        "plt.xlabel('Число кластеров K')\n",
        "plt.ylabel('Inertia (сумма квадратов расстояний)')\n",
        "plt.title('Метод локтя для выбора K')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FrVwRso5twkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Кластеризация KMeans с выбранным K по косинусной метрике\n",
        "n_clusters = 6\n",
        "\n",
        "kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42, n_init='auto')\n",
        "labels_km = kmeans.fit_predict(X_for_clustering)"
      ],
      "metadata": {
        "id": "FvYrlwTDt4bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reducer_2d = umap.UMAP(n_components=2, metric='cosine', random_state=42)\n",
        "X_reduced_2d = reducer_2d.fit_transform(X_all)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "scatter = plt.scatter(X_reduced_2d[:,0], X_reduced_2d[:,1], c=labels_km, cmap='tab20', s=5, alpha=0.6)\n",
        "plt.colorbar(label='KMeans cluster')\n",
        "plt.title(f\"KMeans (cosine) with {n_clusters} clusters\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FhPIC7HzubEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cluster_km'] = labels_km"
      ],
      "metadata": {
        "id": "93ZafQP9xhtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "RfxzlvhDdC01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Тематическое моделирование: c-TF-IDF для ключевых слов кластеров**"
      ],
      "metadata": {
        "id": "JCMNXTEfxhA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "russian_stopwords = stopwords.words(\"russian\")\n",
        "\n",
        "# Формируем DataFrame для кластеров KMeans\n",
        "docs_df_kmeans = pd.DataFrame({\n",
        "    'Doc': df['lemmatized_text'].astype(str),\n",
        "    'Topic': labels_km,\n",
        "    'Doc_ID': range(len(df))\n",
        "})\n",
        "\n",
        "# Объединяем тексты по темам\n",
        "docs_per_topic_kmeans = docs_df_kmeans.groupby(['Topic'], as_index=False).agg({'Doc': ' '.join})"
      ],
      "metadata": {
        "id": "DunDIC680Hw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
        "    count = CountVectorizer(ngram_range=ngram_range,\n",
        "                            stop_words=russian_stopwords).fit(documents)\n",
        "\n",
        "    t = count.transform(documents).toarray()\n",
        "    w = t.sum(axis=1)\n",
        "\n",
        "    tf = np.divide(t.T, w)\n",
        "    sum_t = t.sum(axis=0)\n",
        "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
        "\n",
        "    tf_idf = np.multiply(tf, idf)\n",
        "\n",
        "    return tf_idf, count\n",
        "\n",
        "tf_idf, count = c_tf_idf(docs_per_topic_kmeans.Doc.values, m=len(docs_df_kmeans))"
      ],
      "metadata": {
        "id": "Pnmh-0L30Tl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
        "    words = count.get_feature_names_out()\n",
        "    labels = list(docs_per_topic.Topic)\n",
        "    tf_idf_transposed = tf_idf.T\n",
        "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
        "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
        "    return top_n_words\n",
        "\n",
        "def extract_topic_sizes(df):\n",
        "    topic_sizes = (df.groupby(['Topic'])\n",
        "                     .Doc\n",
        "                     .count()\n",
        "                     .reset_index()\n",
        "                     .rename({\"Topic\": \"Topic\", \"Doc\":  \"Size\"},axis='columns')\n",
        "                     .sort_values(\"Size\", ascending=False))\n",
        "    return topic_sizes\n",
        "\n",
        "top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic_kmeans, n=20)\n",
        "topic_sizes = extract_topic_sizes(docs_df_kmeans); topic_sizes.head(10)"
      ],
      "metadata": {
        "id": "i9cb3FSO03Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for topic_id in topic_sizes['Topic'].head(10):\n",
        "    print(f\"\\nТема {topic_id} (размер = {topic_sizes[topic_sizes['Topic'] == topic_id]['Size'].values[0]})\")\n",
        "    for word, score in top_n_words[topic_id]:\n",
        "        print(f\"{word:<20} {score}\")"
      ],
      "metadata": {
        "id": "VdCsCJuR0-8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression S-score"
      ],
      "metadata": {
        "id": "igUJ-F5VMzcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Формируем матрицу признаков для регрессии: one-hot для кластеров\n",
        "cluster_labels = df['cluster_km'].values.reshape(-1, 1)\n",
        "\n",
        "# Преобразуем в one-hot (каждый кластер - отдельный признак)\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "X_clusters_ohe = enc.fit_transform(cluster_labels)\n",
        "print(\"Размер матрицы one-hot кластеров:\", X_clusters_ohe.shape)"
      ],
      "metadata": {
        "id": "bhC8_teo0_Sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.2. Линейная регрессия: rating ~ cluster_dummies (без intercept)\n",
        "y = df['rating'].values\n",
        "lr = LinearRegression(fit_intercept=False)\n",
        "lr.fit(X_clusters_ohe, y)\n",
        "# коэффициенты для каждого кластера (в порядке возрастания номеров кластеров)\n",
        "coeffs = lr.coef_\n",
        "# Проверим соответствие количества коэффициентов количеству кластеров\n",
        "print(f\"Число коэффициентов регрессии: {len(coeffs)}, число кластеров: {X_clusters_ohe.shape[1]}\")"
      ],
      "metadata": {
        "id": "PVOCd3nN1sG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.3. Нормализация весов тем (коэффициентов) к диапазону [0,1]\n",
        "coef_min = coeffs.min()\n",
        "coef_max = coeffs.max()\n",
        "if coef_max - coef_min == 0:\n",
        "    # На случай, если все коэффициенты равны (что маловероятно) - избежим деления на 0\n",
        "    normalized_weights = np.zeros_like(coeffs)\n",
        "else:\n",
        "    normalized_weights = (coeffs - coef_min) / (coef_max - coef_min)"
      ],
      "metadata": {
        "id": "B71Xj6484H6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.4. Расчет S-score для каждого застройщика\n",
        "# Подготовим словарь весов тем (кластеров)\n",
        "cluster_weights = {cluster: w for cluster, w in enumerate(normalized_weights)}\n",
        "\n",
        "# Определим метки для позитивных и негативных отзывов\n",
        "# Предположительно: sentiment_label = 2 для позитивных, = 0 для негативных (после фильтрации)\n",
        "pos_label = 2\n",
        "neg_label = 0\n",
        "\n",
        "# Считаем общее число отзывов у каждого застройщика (для вычисления долей)\n",
        "total_reviews_by_dev = df.groupby('developer').size().to_dict()\n",
        "\n",
        "s_scores_km = {}  # словарь для S-score\n",
        "# Группируем по разработчику и кластеру с разметкой позитив/негатив\n",
        "pos_counts = df[df['sentiment_label'] == pos_label].groupby(['developer', 'cluster_km']).size().to_dict()\n",
        "neg_counts = df[df['sentiment_label'] == neg_label].groupby(['developer', 'cluster_km']).size().to_dict()\n",
        "\n",
        "developers = df['developer'].unique()\n",
        "for dev in developers:\n",
        "    total = total_reviews_by_dev.get(dev, 0)\n",
        "    if total == 0:\n",
        "        continue\n",
        "    s_value = 0.0\n",
        "    # Для каждого кластера суммируем взвешенную разность долей\n",
        "    for cluster, w in cluster_weights.items():\n",
        "        # Число позитивных и негативных отзывов данного застройщика в этом кластере\n",
        "        pos_count = pos_counts.get((dev, cluster), 0)\n",
        "        neg_count = neg_counts.get((dev, cluster), 0)\n",
        "        # Доли (от общего числа отзывов застройщика)\n",
        "        p_share = pos_count / total\n",
        "        n_share = neg_count / total\n",
        "        # Вклад в S-score\n",
        "        s_value += w * (p_share - n_share)\n",
        "    s_scores_km[dev] = s_value"
      ],
      "metadata": {
        "id": "9o4H4X5d8J0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_score_km_df = pd.DataFrame({\n",
        "    'developer': list(s_scores_km.keys()),\n",
        "    's_score_kmeans': list(s_scores_km.values())\n",
        "})\n",
        "print(\"Пример расчета S-score:\")\n",
        "print(s_score_km_df.head())"
      ],
      "metadata": {
        "id": "wB6hXMP88O9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.6. Оценка качества регрессионной модели\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "y_pred = lr.predict(X_clusters_ohe)\n",
        "r2 = lr.score(X_clusters_ohe, y)\n",
        "mae = mean_absolute_error(y, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
        "print(f\"Качество модели: R^2 = {r2:.3f}, MAE = {mae:.3f}, RMSE = {rmse:.3f}\")"
      ],
      "metadata": {
        "id": "0IJ7AhW98Rec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_score_km_df = s_score_km_df.sort_values(by='s_score_kmeans', ascending=False).reset_index(drop=True)\n",
        "s_score_km_df"
      ],
      "metadata": {
        "id": "vxZ7rMsD9yX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### S-score with log-regression"
      ],
      "metadata": {
        "id": "UAh_mg9WmRfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from scipy.special import logit"
      ],
      "metadata": {
        "id": "62KzKDZjmwHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Получаем признаки\n",
        "X = df[['cluster_km']]\n",
        "y = df['sentiment_label']\n",
        "\n",
        "# One-hot кодируем кластеры\n",
        "encoder = OneHotEncoder(sparse_output=False, drop=None)\n",
        "X_encoded = encoder.fit_transform(X)"
      ],
      "metadata": {
        "id": "8Y7eRkkNmoIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучаем логистическую регрессию\n",
        "logreg = LogisticRegression(fit_intercept=False, solver='liblinear')\n",
        "logreg.fit(X_encoded, y)\n",
        "\n",
        "# Получаем коэффициенты\n",
        "cluster_coeffs = logreg.coef_[0]\n",
        "cluster_weights_logreg = {i: coeff for i, coeff in enumerate(cluster_coeffs)}"
      ],
      "metadata": {
        "id": "4sVIcxpImqqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Подсчет количества отзывов по застройщикам и кластерам\n",
        "pos_label = 2\n",
        "neg_label = 0\n",
        "\n",
        "total_reviews_by_dev = df.groupby('developer').size().to_dict()\n",
        "pos_counts = df[df['sentiment_label'] == pos_label].groupby(['developer', 'cluster_km']).size().to_dict()\n",
        "neg_counts = df[df['sentiment_label'] == neg_label].groupby(['developer', 'cluster_km']).size().to_dict()"
      ],
      "metadata": {
        "id": "HyRJ9zBymsxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Расчет S-score логистической\n",
        "s_scores_logreg = {}\n",
        "developers = df['developer'].unique()\n",
        "for dev in developers:\n",
        "    total = total_reviews_by_dev.get(dev, 0)\n",
        "    if total == 0:\n",
        "        continue\n",
        "    s_value = 0.0\n",
        "    for cluster, w in cluster_weights_logreg.items():\n",
        "        pos_count = pos_counts.get((dev, cluster), 0)\n",
        "        neg_count = neg_counts.get((dev, cluster), 0)\n",
        "        p_share = pos_count / total\n",
        "        n_share = neg_count / total\n",
        "        s_value += w * (p_share - n_share)\n",
        "    s_scores_logreg[dev] = s_value\n",
        "\n",
        "# Нормализация\n",
        "s_values = np.array(list(s_scores_logreg.values()))\n",
        "s_values_norm = (s_values - s_values.min()) / (s_values.max() - s_values.min())"
      ],
      "metadata": {
        "id": "mRb-TSBfnCsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_score_df_logreg = pd.DataFrame({\n",
        "    'developer': list(s_scores_logreg.keys()),\n",
        "    's_score_logreg': s_values_norm\n",
        "})\n",
        "\n",
        "s_score_df_logreg.sort_values(by='s_score_logreg', ascending=False).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "2a16QNBYnFgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Средний рейтинг по девелоперам\n",
        "mean_ratings = df.groupby('developer')['rating'].mean().reset_index()\n",
        "mean_ratings.columns = ['developer', 'avg_rating']\n",
        "\n",
        "s_min = s_score_df_logreg['s_score_logreg'].min()\n",
        "s_max = s_score_df_logreg['s_score_logreg'].max()\n",
        "s_score_df_logreg['s_sco=re_logreg_scaled'] = 5 * (s_score_df_logreg['s_score_logreg'] - s_min) / (s_max - s_min)\n",
        "\n",
        "# Объединяем с индексом\n",
        "validation_df = s_score_df_logreg.merge(mean_ratings, on='developer')\n",
        "validation_df.sort_values(by='avg_rating', ascending=False).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "GIV1YyFE2hKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "y_pred = logreg.predict_proba(X_encoded)[:, 1]\n",
        "\n",
        "# McFadden's Pseudo R²\n",
        "epsilon = 1e-15\n",
        "y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "\n",
        "p_null = np.mean(y)\n",
        "p_null = np.clip(p_null, epsilon, 1 - epsilon)  # защита от log(0)\n",
        "\n",
        "log_likelihood_full = np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
        "log_likelihood_null = np.sum(y * np.log(p_null) + (1 - y) * np.log(1 - p_null))\n",
        "pseudo_r2 = 1 - (log_likelihood_full / log_likelihood_null)\n",
        "print(f\"McFadden's Pseudo R²: {pseudo_r2:.4f}\")\n",
        "\n",
        "# ROC - AUC\n",
        "auc = roc_auc_score(y, y_pred)\n",
        "print(f\"ROC-AUC: {auc:.4f}\")\n",
        "\n",
        "# Gini\n",
        "gini = 2 * auc - 1\n",
        "print(f\"Gini: {gini:.4f}\")\n",
        "\n",
        "# Корреляция Спирмана\n",
        "y_pred_val = validation_df['s_score_logreg_scaled']\n",
        "y_true_val = validation_df['avg_rating']\n",
        "r_spear, _ = spearmanr(y_true_val, y_pred_val)\n",
        "print(f\"Spearman rank correlation between normalized s-score and average rating: {r_spear:.3f}\")"
      ],
      "metadata": {
        "id": "dooI1LO2nJxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Строим scatterplot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(\n",
        "    data=validation_df,\n",
        "    x='s_score_logreg_scaled',\n",
        "    y='avg_rating',\n",
        "    ci=None,\n",
        "    scatter_kws={'s': 50, 'alpha': 0.8},\n",
        "    line_kws={'color': 'red'}\n",
        ")\n",
        "plt.title(\"Correlation between S-score and average rating of developer (Spearman)\", fontsize=14)\n",
        "plt.xlabel(\"Normilized S-score\", fontsize=12)\n",
        "plt.ylabel(\"Average rating (1–5)\", fontsize=12)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EfAxQBBg8iXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HDBSCAN clusterisation"
      ],
      "metadata": {
        "id": "EBGl0VryQo5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "4Xo0W0BoQ5Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hdbscan\n",
        "import optuna\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "X_for_hdbscan = normalize(X_reduced_100)\n",
        "\n",
        "# Вариант 1: Подбор через Optuna (максимизация silhouette_score)\n",
        "def objective(trial):\n",
        "    min_cluster_size = trial.suggest_int(\"min_cluster_size\", 10, 50)\n",
        "    min_samples = trial.suggest_int(\"min_samples\", 1, 20)\n",
        "\n",
        "    clusterer = hdbscan.HDBSCAN(\n",
        "        min_cluster_size=min_cluster_size,\n",
        "        min_samples=min_samples,\n",
        "        metric=\"euclidean\"\n",
        "    )\n",
        "    labels = clusterer.fit_predict(X_for_hdbscan)\n",
        "\n",
        "    mask = labels != -1\n",
        "    if len(set(labels[mask])) < 2:\n",
        "        return -1.0\n",
        "\n",
        "    score = silhouette_score(X_for_hdbscan[mask], labels[mask])\n",
        "    return score\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=30)\n",
        "\n",
        "best_params = study.best_params"
      ],
      "metadata": {
        "id": "7RhLJHsV52yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best hyperparameters for silhouette:\", study.best_params)"
      ],
      "metadata": {
        "id": "m6ZCndegQzro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_min_noise():\n",
        "    best_ratio = 1.0\n",
        "    best_config = None\n",
        "\n",
        "    for min_cluster_size in range(10, 51, 5):\n",
        "        for min_samples in [1, 3, 5, 10, 15, 20]:\n",
        "            clusterer = hdbscan.HDBSCAN(\n",
        "                min_cluster_size=min_cluster_size,\n",
        "                min_samples=min_samples,\n",
        "                metric='euclidean'\n",
        "            )\n",
        "            labels = clusterer.fit_predict(X_for_hdbscan)\n",
        "            noise_ratio = np.mean(labels == -1)\n",
        "\n",
        "            if noise_ratio < best_ratio and len(set(labels)) > 2:\n",
        "                best_ratio = noise_ratio\n",
        "                best_config = {\n",
        "                    \"min_cluster_size\": min_cluster_size,\n",
        "                    \"min_samples\": min_samples,\n",
        "                    \"noise_ratio\": noise_ratio\n",
        "                }\n",
        "\n",
        "    return best_config\n",
        "\n",
        "best_noise_config = optimize_min_noise()"
      ],
      "metadata": {
        "id": "JViT0K98U2cZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best hyperparameters for noise minimization:\", best_noise_config)"
      ],
      "metadata": {
        "id": "yLB6tO6BU6Do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_for_hdbscan = X_reduced_100\n",
        "\n",
        "X_for_hdbscan = normalize(X_for_hdbscan)\n",
        "\n",
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=15, min_samples=9, metric='euclidean')\n",
        "labels_hdb = clusterer.fit_predict(X_for_hdbscan)\n",
        "n_clusters_hdb = len(set(labels_hdb) - {-1})\n",
        "print(f\"HDBSCAN нашел кластеров: {n_clusters_hdb}, выбросов: {(labels_hdb == -1).sum()}\")"
      ],
      "metadata": {
        "id": "HpAV9mUddEJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.3. Визуализация кластеров HDBSCAN на 2D проекции\n",
        "plt.figure(figsize=(8,6))\n",
        "outlier_mask = (labels_hdb == -1)\n",
        "# Точки-выбросы\n",
        "plt.scatter(X_reduced_2d[outlier_mask,0], X_reduced_2d[outlier_mask,1], c='lightgray', s=5, alpha=0.5, label='outliers')\n",
        "# Точки кластеров\n",
        "plt.scatter(X_reduced_2d[~outlier_mask,0], X_reduced_2d[~outlier_mask,1], c=labels_hdb[~outlier_mask], cmap='tab20', s=5, alpha=0.6)\n",
        "plt.colorbar()\n",
        "plt.title('Кластеры HDBSCAN (UMAP 2D проекция)')\n",
        "plt.xlabel('UMAP 1')\n",
        "plt.ylabel('UMAP 2')\n",
        "plt.legend(loc='upper right')\n",
        "plt.savefig('hdbscan_clusters_2d.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yE_Yl63dazCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "russian_stopwords = stopwords.words(\"russian\")\n",
        "\n",
        "# Формируем DataFrame для кластеров KMeans\n",
        "docs_df_hdbscan = pd.DataFrame({\n",
        "    'Doc': df['lemmatized_text'].astype(str),\n",
        "    'Topic': labels_hdb,\n",
        "    'Doc_ID': range(len(df))\n",
        "})\n",
        "\n",
        "# Объединяем тексты по темам\n",
        "docs_per_topic_hdbscan = docs_df_hdbscan.groupby(['Topic'], as_index=False).agg({'Doc': ' '.join})"
      ],
      "metadata": {
        "id": "tHV1ZrHReVHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
        "    count = CountVectorizer(ngram_range=ngram_range,\n",
        "                            stop_words=russian_stopwords).fit(documents)\n",
        "\n",
        "    t = count.transform(documents).toarray()\n",
        "    w = t.sum(axis=1)\n",
        "\n",
        "    tf = np.divide(t.T, w)\n",
        "    sum_t = t.sum(axis=0)\n",
        "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
        "\n",
        "    tf_idf = np.multiply(tf, idf)\n",
        "\n",
        "    return tf_idf, count\n",
        "\n",
        "tf_idf_hdbscan, count_hdbscan = c_tf_idf(docs_per_topic_hdbscan.Doc.values, m=len(docs_df_hdbscan))"
      ],
      "metadata": {
        "id": "qcgaxQeZe1ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
        "    words = count.get_feature_names_out()\n",
        "    labels = list(docs_per_topic.Topic)\n",
        "    tf_idf_transposed = tf_idf.T\n",
        "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
        "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
        "    return top_n_words\n",
        "\n",
        "def extract_topic_sizes(df):\n",
        "    topic_sizes = (df.groupby(['Topic'])\n",
        "                     .Doc\n",
        "                     .count()\n",
        "                     .reset_index()\n",
        "                     .rename({\"Topic\": \"Topic\", \"Doc\":  \"Size\"},axis='columns')\n",
        "                     .sort_values(\"Size\", ascending=False))\n",
        "    return topic_sizes\n",
        "\n",
        "top_n_words_hdbscan = extract_top_n_words_per_topic(tf_idf_hdbscan, count_hdbscan, docs_per_topic_hdbscan, n=20)\n",
        "topic_sizes_hdbscan = extract_topic_sizes(docs_df_hdbscan); topic_sizes_hdbscan.head(10)"
      ],
      "metadata": {
        "id": "8bB4TqztfCwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for topic_id in topic_sizes_hdbscan['Topic'].head(10):\n",
        "    print(f\"\\nТема {topic_id} (размер = {topic_sizes_hdbscan[topic_sizes_hdbscan['Topic'] == topic_id]['Size'].values[0]})\")\n",
        "    for word, score in top_n_words_hdbscan[topic_id]:\n",
        "        print(f\"{word:<20} {score}\")"
      ],
      "metadata": {
        "id": "hx0-H8GYfDOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"cluster_hdb\"] = labels_hdb"
      ],
      "metadata": {
        "id": "HNuRlEAOhY9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot представление кластеров HDBSCAN для регрессии\n",
        "cluster_labels_hdb = df['cluster_hdb'].values.reshape(-1, 1)\n",
        "X_clusters_hdb_ohe = enc.fit_transform(cluster_labels_hdb)\n",
        "print(\"One-hot форма кластеров HDBSCAN:\", X_clusters_hdb_ohe.shape)"
      ],
      "metadata": {
        "id": "w16D7bCxhTP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Линейная регрессия: rating ~ cluster_hdb (OHE), без intercept\n",
        "y_hdb = df['rating'].values\n",
        "lr_hdb = LinearRegression(fit_intercept=False)\n",
        "lr_hdb.fit(X_clusters_hdb_ohe, y_hdb)\n",
        "coeffs_hdb = lr_hdb.coef_\n",
        "print(f\"Количество кластеров HDBSCAN: {X_clusters_hdb_ohe.shape[1]}, коэффициентов: {len(coeffs_hdb)}\")"
      ],
      "metadata": {
        "id": "lYc8DV2cilod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Нормализация весов тем HDBSCAN\n",
        "coef_min_hdb = coeffs_hdb.min()\n",
        "coef_max_hdb = coeffs_hdb.max()\n",
        "if coef_max_hdb - coef_min_hdb == 0:\n",
        "    normalized_weights_hdb = np.zeros_like(coeffs_hdb)\n",
        "else:\n",
        "    normalized_weights_hdb = (coeffs_hdb - coef_min_hdb) / (coef_max_hdb - coef_min_hdb)\n",
        "cluster_weights_hdb = {cluster: w for cluster, w in enumerate(normalized_weights_hdb)}"
      ],
      "metadata": {
        "id": "XtU9TCk9ivvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Расчет S-score для каждой компании (HDBSCAN)\n",
        "total_reviews_by_dev_hdb = df.groupby('developer').size().to_dict()\n",
        "pos_counts_hdb = df[df['sentiment_label'] == pos_label].groupby(['developer', 'cluster_hdb']).size().to_dict()\n",
        "neg_counts_hdb = df[df['sentiment_label'] == neg_label].groupby(['developer', 'cluster_hdb']).size().to_dict()\n",
        "\n",
        "s_scores_hdb = {}\n",
        "for dev in df['developer'].unique():\n",
        "    total = total_reviews_by_dev_hdb.get(dev, 0)\n",
        "    if total == 0:\n",
        "        continue\n",
        "    s_value = 0.0\n",
        "    for cluster, w in cluster_weights_hdb.items():\n",
        "        pos_count = pos_counts_hdb.get((dev, cluster), 0)\n",
        "        neg_count = neg_counts_hdb.get((dev, cluster), 0)\n",
        "        p_share = pos_count / total\n",
        "        n_share = neg_count / total\n",
        "        s_value += w * (p_share - n_share)\n",
        "    s_scores_hdb[dev] = s_value\n",
        "\n",
        "s_score_hdb_df = pd.DataFrame({\n",
        "    'developer': list(s_scores_hdb.keys()),\n",
        "    's_score_hdbscan': list(s_scores_hdb.values())\n",
        "})\n",
        "print(\"Пример S-score (HDBSCAN) - топ 5 записей:\")\n",
        "print(s_score_hdb_df.head())"
      ],
      "metadata": {
        "id": "C3-vGMWriyec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_score_hdb_df.sort_values(by='s_score_hdbscan', ascending=False).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "A2vVt7dTj0ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_hdb = lr_hdb.predict(X_clusters_hdb_ohe)\n",
        "r2_hdb = lr_hdb.score(X_clusters_hdb_ohe, y_hdb)\n",
        "mae_hdb = mean_absolute_error(y_hdb, y_pred_hdb)\n",
        "rmse_hdb = np.sqrt(mean_squared_error(y_hdb, y_pred_hdb))\n",
        "print(f\"Качество модели (HDBSCAN кластеры): R^2 = {r2_hdb:.3f}, MAE = {mae_hdb:.3f}, RMSE = {rmse_hdb:.3f}\")"
      ],
      "metadata": {
        "id": "LI_SHQe9jGxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ftFUEIgVlVkl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}